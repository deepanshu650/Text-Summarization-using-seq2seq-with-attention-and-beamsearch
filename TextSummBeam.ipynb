{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/deepanshu/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data , vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import fastai\n",
    "from fastai.text import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## returns corresponding lib with default language as en\n",
    "tokenizer = data.get_tokenizer('spacy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defines datatype that makes tensor and have vocab object\n",
    "TEXT = data.Field(tokenize = tokenizer , lower = True , eos_token = '_eos_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset used is gigaword dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data_fields = [(\"source\" , TEXT),\n",
    "                   (\"target\" , TEXT)]\n",
    "\n",
    "trn , vld = data.TabularDataset.splits(path = r'/home/deepanshu/Downloads/' ,train = 'GigaTrainSet.csv' ,\n",
    "                                       validation = 'GigaValdSet.csv' ,format = 'csv' , skip_header=True , \n",
    "                                       fields = trn_data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tuareg', 'rebels', 'in', 'mali', 'free', 'last', 'hostages'] ['tuareg', 'rebel', 'chief', 'ibrahim', 'ag', 'bahanga', 'has', 'released', 'the', 'last', '#', '#', 'hostages', 'held', 'by', 'his', 'renegade', 'group', 'in', 'the', 'north', 'of', 'mali', ',', 'the', 'government', 'has', 'said', ',', 'in', 'an', 'important', 'step', 'forward', 'towards', 'peace', '.']\n"
     ]
    }
   ],
   "source": [
    "print(trn[50].source , trn[50].target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab dictionary is made by highest frequency and alphabetic order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_vector_type='glove.6B.200d'\n",
    "TEXT.build_vocab(trn , vectors = pre_trained_vector_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 150346),\n",
       " ('the', 124206),\n",
       " ('.', 95927),\n",
       " ('in', 81764),\n",
       " (',', 81572),\n",
       " ('to', 80665),\n",
       " ('of', 75256),\n",
       " ('a', 70129),\n",
       " ('on', 49036),\n",
       " ('-', 40212)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterator that loads batches of data from a Dataset\n",
    "batch_size =32\n",
    "train_iter  = data.BucketIterator( trn  , batch_size = batch_size, sort_key = lambda x : len(x.target) ,\n",
    "                                    shuffle = True , sort_within_batch=False , repeat = False)\n",
    "valid_iter = data.BucketIterator( vld  , batch_size = batch_size, sort_key = lambda x : len(x.target) ,\n",
    "                                    shuffle = True , sort_within_batch=False , repeat = False)\n",
    "                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.source]:[torch.LongTensor of size 18x32]\n",
       "\t[.target]:[torch.LongTensor of size 54x32]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.source]:[torch.LongTensor of size 21x32]\n",
       "\t[.target]:[torch.LongTensor of size 50x32]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(valid_iter.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchTuple():\n",
    "    def __init__(self , dataset , x_var , y_var):\n",
    "        self.dataset , self.x_var , self.y_var = dataset , x_var , y_var\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.dataset:\n",
    "            x = getattr(batch , self.x_var)\n",
    "            y = getattr(batch , self.y_var)\n",
    "            yield(x ,y)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_iter_tuple = BatchTuple(train_iter , \"source\" ,\"target\")\n",
    "valid_iter_tuple = BatchTuple(valid_iter , \"source\" , \"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1105,   172,   639,  1850,  3790,  2262,    64,  4807,  1715,    83,\n",
       "            305,   329,  1223,  2014,   570,   844,  5783,  4185,  3174,  5027,\n",
       "             50,  3374,  3830,    64,    69,  2940,   865,   129,   146,    82,\n",
       "            134,  3108],\n",
       "         [   81,     6,     9,     9,     8,   457,   216,    39,  5398, 16870,\n",
       "              9,   127,   591,  1419,  1215,   693,   700,  2548,  3093, 14908,\n",
       "            196,   675,  5057,   216,  2123,    17,     8,   156,   162,   617,\n",
       "            878,   246],\n",
       "         [    6,   455,   133,  3162,  6419,     6,  1982,   131,   176,  2338,\n",
       "           1085,  2303,   142,  1838,   328,  2443,    16,     6,    79,    51,\n",
       "            161,  2005,    56,  1982,  3093,    50,   946,  2507,  5591,  7906,\n",
       "              8,  6390],\n",
       "         [  601,    40,    13,    44,   133,   339,   159,   322,   484,   223,\n",
       "            166,     3,   301,   175,    16,    16,  4596,     8,    11,   261,\n",
       "             64,    11,  2169,   159,   148,    72,   201,     6,    19,   966,\n",
       "           2202,   832],\n",
       "         [ 5913,     3,  1273,  5743,     3,  1126,  1462,   275,  3413,     3,\n",
       "            101,     5,   693,  2424,    10,  6010,  1912,  5045, 13743,  6466,\n",
       "            216,  2116,   118,  1462,    78,   501,     9,   638, 14450,   200,\n",
       "            600,     6],\n",
       "         [  539,     3,  7327,    16,     5,   118,     2,    49,     3,     3,\n",
       "              8,     3,     2,   681,   471,   686,     2,   194,    69,    16,\n",
       "             13,   849, 11250,     2,  1783,   111,   284,    75,  1182,   168,\n",
       "            346,   390],\n",
       "         [   40,     3,     6,   197,     3,  4867,     1,    63,     3,     5,\n",
       "            198,     3,     1,     2,   481,  1183,     1,   498,  1274,   361,\n",
       "            422,   229,  3249,     1,     9,   598,   112,   147,  2592,    91,\n",
       "             16,   142],\n",
       "         [ 2384,    78,  2706,   126,   164,    16,     1,  1458,     2,     3,\n",
       "             53,    34,     1,     1,    74,     2,     1,   345, 11791,     2,\n",
       "           4241,     2,   149,     1,  4531,     2,     2,    23,     3,   908,\n",
       "            355,   869],\n",
       "         [18382,   116,    23, 13131,    80,   135,     1,     2,     1,    34,\n",
       "              2,  3273,     1,     1,     2,     1,     1,  1698,     2,     1,\n",
       "              2,     1,   823,     1,     2,     1,     1,  1605,     3,     2,\n",
       "             13,     2],\n",
       "         [    2,   319,   179,     2,     6,    76,     1,     1,     1,     2,\n",
       "              1,    23,     1,     1,     1,     1,     1,     2,     1,     1,\n",
       "              1,     1,     2,     1,     1,     1,     1,     2,     3,     1,\n",
       "           1098,     1],\n",
       "         [    1,   225,   840,     1,   114,     2,     1,     1,     1,     1,\n",
       "              1,   198,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     2,     1,\n",
       "              2,     1],\n",
       "         [    1,     2,     2,     1,    73,     1,     1,     1,     1,     1,\n",
       "              1,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1],\n",
       "         [    1,     1,     1,     1,   157,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1],\n",
       "         [    1,     1,     1,     1,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1]]),\n",
       " tensor([[1105,  172,   18,  ...,   22,    4,    4],\n",
       "         [  81,   72,  133,  ...,   82,  134,  965],\n",
       "         [   7,  150,   13,  ...,  174,  411,   15],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_iter_tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SampleDataPath = r'/home/deepanshu/Downloads/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 32, 41230)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter_tuple) , len(valid_iter_tuple) , len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21, 32]), torch.Size([53, 32]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t ,z = next(train_iter_tuple.__iter__())\n",
    "t.size() , z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Source \n",
      "economic outlook pushes australian shares # . # percent higher _eos_ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "Corresponding Tensor \n",
      "[ 192 1320 4540  151   77    3    5    3   34  110    2    1    1    1    1    1    1    1    1    1    1] \n",
      "\n",
      "\n",
      "Sample Target \n",
      "australian shares prices rose by almost half a percent wednesday , buoyed by a healthy outlook for the domestic economy and rises in overseas markets , brokers said . _eos_ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "Corresponding Tensor \n",
      "[151  77  58 360 ...   1   1   1   1] \n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "sample_source = t.transpose(1,0)[0].data.cpu().numpy()\n",
    "sample_target = z.transpose(1,0)[0].data.cpu().numpy()\n",
    "\n",
    "print(\"Sample Source \\n%s \\n\\nCorresponding Tensor \\n%s \\n\\n\" %(' '.join([TEXT.vocab.itos[o] for o in sample_source])\n",
    "                                                               , sample_source))\n",
    "print(\"Sample Target \\n%s \\n\\nCorresponding Tensor \\n%s \\n\\n \" %(' '.join([TEXT.vocab.itos[o] for o in sample_target])\n",
    "                                                               , sample_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#### Model\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self , input_size , embz_size , hidden_size , output_size , batch_size , max_tgt_len ,\n",
    "                 pre_trained_vector ,pre_trained_vector_type ,padding_idx , force_prob = 0.7,\n",
    "                 encoder_drop =(0,0,0,0) , decoder_drop =(0,0,0,0),num_layers =2 ,\n",
    "                 teacher_forcing = True , bias = False):\n",
    "        super().__init__()\n",
    "        self.output_size , self.embz_size , self.hidden_size = output_size ,embz_size ,hidden_size\n",
    "        \n",
    "        self.num_layers = num_layers \n",
    "        self.max_tgt_len , self.input_size , self.pretrained_vectors = max_tgt_len ,input_size ,pre_trained_vector\n",
    "        \n",
    "        self.teacher_forcing , self.pre_trained_vector_type = teacher_forcing , pre_trained_vector_type\n",
    "        \n",
    "        self.encoder_drop , self.decoder_drop , self.padding_idx = encoder_drop ,decoder_drop, padding_idx\n",
    "        \n",
    "        if self.teacher_forcing: \n",
    "            self.force_prob = force_prob\n",
    "        \n",
    "        self.bidirectional = True\n",
    "        ## since bidirectional\n",
    "        self.num_directions = 2\n",
    "        self.beam_width = 5\n",
    "      \n",
    "            \n",
    "        #encoder_\n",
    "        self.encoder_dropout = nn.Dropout(self.encoder_drop[0])\n",
    "        self.encoder_embedding_layer = nn.Embedding(self.input_size ,self.embz_size , padding_idx=self.padding_idx)\n",
    "        if self.pretrained_vectors :\n",
    "            self.encoder_embedding_layer.weight.data.copy_(self.pretrained_vectors.weight.data)\n",
    "            \n",
    "        self.encoder_LSTM = nn.LSTM(input_size = self.embz_size, hidden_size = self.hidden_size,\n",
    "                                     num_layers = self.num_layers ,dropout = self.encoder_drop[1],\n",
    "                                    bidirectional = True\n",
    "                                    )       \n",
    "        \n",
    "        self.encoder_vector_layer = nn.Linear(self.hidden_size*self.num_directions ,self.embz_size , bias=bias)\n",
    "        \n",
    "        ##decoder\n",
    "        self.decoder_dropout = nn.Dropout(self.decoder_drop[0])\n",
    "        self.decoder_embedding_layer = nn.Embedding(self.input_size , self.embz_size , padding_idx=self.padding_idx)\n",
    "        if self.pretrained_vectors :\n",
    "            self.decoder_embedding_layer.weight.data.copy_(self.pretrained_vectors.weight.data)\n",
    "        \n",
    "        self.decoder_LSTM = nn.LSTM(input_size = self.embz_size, hidden_size = self.hidden_size*self.num_directions,\n",
    "                                     num_layers = 1 ,dropout = self.decoder_drop[1],\n",
    "                                    bidirectional = False)\n",
    "        self.decoder_output_layer = nn.Linear(self.hidden_size*self.num_directions ,self.embz_size , bias=bias)\n",
    "        self.output_layer = nn.Linear(self.embz_size , self.output_size , bias =bias)\n",
    "        \n",
    "        ##set attention\n",
    "        self.encoder_output_layer =  nn.Linear(self.hidden_size*self.num_directions ,self.embz_size , bias=bias)\n",
    "        self.attention_vector_layer = nn.Linear(self.embz_size*2 , self.embz_size , bias = bias)\n",
    "        self.decoder_hidden_layer = nn.Linear(self.hidden_size*self.num_directions ,self.embz_size , bias=bias)\n",
    "        self.attn_score = nn.Linear(self.embz_size , 1 , bias=bias)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self , batch_size):\n",
    "        return (torch.zeros(self.num_layers*self.num_directions , batch_size , self.hidden_size) ,\n",
    "                    torch.zeros(self.num_layers*self.num_directions , batch_size , self.hidden_size) )\n",
    "       \n",
    "    \n",
    "    def cat_directions(self , hidden):\n",
    "        def cat_(h):\n",
    "            #changes 2*l into l(layers) and concat at hidden_size (3rd dim.) so that we can extract last layer\n",
    "            return torch.cat([h[0:h.size(0):2] , h[1:h.size(0):2]] ,2 )\n",
    "            \n",
    "        hidden = tuple([cat_(h) for h in hidden])\n",
    "        return hidden\n",
    "    \n",
    "    \n",
    "    def attention(self ,encoder_output,decoder_hidden,decoder_input):\n",
    "        encoder_out = self.encoder_output_layer(encoder_output)  #(seqlen , batch , embz)\n",
    "        encoder_out = encoder_out.transpose(1,0)                 #(batch , seqlen , embz)\n",
    "        decoder_hidden = decoder_hidden.transpose(1,0)          # (batch , 1 ,embz)\n",
    "        att_score = torch.tanh(encoder_out + decoder_hidden)\n",
    "        att_score = self.attn_score(att_score)                   #(batch , seqlen ,1)\n",
    "        att_weight = F.softmax(att_score , dim=1)\n",
    "        context_vector = torch.bmm(att_weight.transpose(-1,1) ,encoder_out ).squeeze(1)  #(batch  ,embz)\n",
    "        att_vector = torch.cat((context_vector ,decoder_input ) , dim=1)                 #(batch , 2*embz)\n",
    "        att_vector = self.attention_vector_layer(att_vector)                             #(batch , embz)\n",
    "        return att_weight.squeeze(-1) , att_vector\n",
    "            \n",
    "    \n",
    "    # Make one step of decoder lstm \n",
    "        \n",
    "    def dec_lstm_step(self , dec_input , encoder_output , decoder_hidden ) :\n",
    "        dec_input = self.decoder_dropout(self.decoder_embedding_layer(dec_input))\n",
    "        prev_hidden = self.decoder_hidden_layer(decoder_hidden[0][-1]).unsqueeze(0)\n",
    "        #(1,batch,embz)       (batch,embz)      last layer of (l,batch,2*hidden_size)\n",
    "        att , decoder_input = self.attention(encoder_output , prev_hidden , dec_input)\n",
    "        #(batch , seqlen)  , (batch ,  embz)\n",
    "        decoder_hidden = (decoder_hidden[0][-1].unsqueeze(0) ,decoder_hidden[1][-1].unsqueeze(0) )  \n",
    "        #give the top layer states to decoder_LSTM\n",
    "        decoder_output , decoder_hidden = self.decoder_LSTM(decoder_input.unsqueeze(0) , decoder_hidden)\n",
    "        #(1,batch,2*hidden) ((1,batch,2*hidden_size),(1,batch,2*hidden_size))\n",
    "        \n",
    "        decoder_output = self.decoder_output_layer(decoder_output.squeeze(0))\n",
    "        #(batch , embz)\n",
    "        output = self.output_layer(decoder_output)   #(batch , output_size) \n",
    "        output = F.softmax(output , dim=1)\n",
    "        return att , output ,decoder_hidden\n",
    "    \n",
    "    \n",
    "    # for training \n",
    "        \n",
    "    def decoder_forward(self , batch_size, encoder_output , decoder_hidden , force_prob =0.7, y=None ):\n",
    "        self.force_prob = force_prob\n",
    "        dec_input = torch.zeros(batch_size).long()\n",
    "        output_seq_stack , att_stack = [] , []\n",
    "        for i in range(self.max_tgt_len):\n",
    "            att , output , decoder_hidden = self.dec_lstm_step( dec_input , encoder_output , decoder_hidden )\n",
    "            \n",
    "            att_stack.append(att)\n",
    "            \n",
    "            output_seq_stack.append(output)\n",
    "            decoder_input = output.data.max(1)[1]     #(batch)   \n",
    "            \n",
    "            if self.teacher_forcing:\n",
    "                samp_prob = round(random.random() ,1)\n",
    "                if (y is not None) and (samp_prob<self.force_prob):     #condition y not None is always true here\n",
    "                    if i >= len(y):                                                    \n",
    "                        break\n",
    "                    decoder_input = y[i]\n",
    "                    \n",
    "            if (decoder_input==1).all(): \n",
    "                break \n",
    "            \n",
    "        return torch.stack(output_seq_stack) , torch.stack(att_stack)  \n",
    "                   #(max_tgt_len,batch,output_size)  , (max_tgt_len,batch,seqlen)  \n",
    "                \n",
    "    \n",
    "    \n",
    "    # for inference \n",
    "    \n",
    "    def beam_search_inference(self, batch_size , beam_width, encoder_output , decoder_hidden):\n",
    "        dec_input = torch.zeros(batch_size).long()                           \n",
    "        final_score , sent , indices_list = [] ,[] ,[]\n",
    "        minus_inf_output = torch.ones(batch_size , output_size)*(-float(\"Inf\"))\n",
    "        att , output,decoder_hidden = self.dec_lstm_step(dec_input, encoder_output , decoder_hidden)  \n",
    "        #output.size()=(batch,output_size) decoder_hidden.size()=((1,batch,2*hidden_size),(1,batch,2*hidden_size))\n",
    "        h,c = decoder_hidden\n",
    "        h_list = [h.squeeze(0) for k in range(beam_width)]              #each element of list is h squeezed (batch,2*hiiden_size)\n",
    "        c_list = [c.squeeze(0) for k in range(beam_width)]\n",
    "        \n",
    "        score = torch.log(output.topk(beam_width , dim =1)[0])          #take log of prob of top words\n",
    "        \n",
    "        for i in range(self.max_tgt_len -1):                            # -1 as one step has already been taken\n",
    "            indices = output.topk(beam_width , dim =1)[1]               #get indices with top prob.(batch,beam_width)\n",
    "            indices_source = torch.floor((indices)*(1/output_size))     # get the source of top scores\n",
    "            indices_char = (indices)%(output_size)                      # get chars of top scorers\n",
    "            char_source = zip(indices_source ,indices_char)             #zip the source and chars so that later \n",
    "            indices_list.append(char_source)                            #sentence can be backtracked \n",
    "            \n",
    "            beam_score , decoder_hidden_list= [] ,[]                                             \n",
    "            for j in range(beam_width):                              \n",
    "                dec_input = indices_char[:,j]                           #give each top indices character as input one by one\n",
    "                if(dec_input ==1).all():                                #if input is <pad>\n",
    "                    score[:,j] = score[:,j]*(1/(i+1))                   #normalize the score by length(+1 to include length till <pad>)\n",
    "                                                                        #as the score contains <pad> prob also\n",
    "                    final_score.append(score[:,j])                      #make list of all scores so that can be compared later\n",
    "                    sent.append((i,j))                                  #take note of (i,j) for sentence retrivel\n",
    "                    beam_score.append(minus_inf_output)                 #append a dummy output having -inf log(prob) and  \n",
    "                                                                        #decoder hidden_state with None so that\n",
    "                    decoder_hidden_list.append((None,None))             #source can be assigned properly to other input  \n",
    "                    \n",
    "                    continue\n",
    "                \n",
    "                ind_src = indices_source[:,j]                           #get the source of the dec_input for decoder_hidden\n",
    "                temp_h = [h_list[int(ind_src[bt])][bt]  for bt in range(batch_size)]\n",
    "                temp_c = [c_list[int(ind_src[bt])][bt]  for bt in range(batch_size)]\n",
    "                #make a list of tensors of the corresponding decoder_hidden states for each element of batch\n",
    "                \n",
    "                h = torch.stack(temp_h)                                #make it tensor (1,batch,2*hidden_state)\n",
    "                h = h.unsqueeze(0)\n",
    "                \n",
    "                c = torch.stack(temp_c)                                #so that it can be passed to lstm\n",
    "                c = c.unsqueeze(0)\n",
    "                \n",
    "                decoder_hidden = (h,c)\n",
    "                att , output ,decoder_hidden_new = self.dec_lstm_step(dec_input , encoder_output , decoder_hidden)\n",
    "                \n",
    "                decoder_hidden_list.append(decoder_hidden_new)         #list of tuples \n",
    "                output = torch.log(output)                             #log of the new outputs given j-th indices as input \n",
    "                sc_output = output + score[:,j].unsqueeze(1)           #add the previous log prob to it\n",
    "                beam_score.append(sc_output)                           #make a list of all possible outputs so that top\n",
    "                                                                       #prob can be extracted among all inputs\n",
    "                \n",
    "            output = torch.cat(beam_score , dim =1)                    #make a tensor from the list\n",
    "            if (output==-float(\"Inf\")).all():                          #if every input is <pad> then break because no new\n",
    "                break                                                  #output will be genrated and score will be divided\n",
    "                                                                       # by i+1 again and again\n",
    "            score = output.topk(beam_width , dim =1)[0]                #find the top scores among all possible output\n",
    "            h_list = [decoder_hidden_list[k][0].squeeze(0) if decoder_hidden_list[k][0] is not None  else None   for k in range(beam_width)]\n",
    "            c_list = [decoder_hidden_list[k][1].squeeze(0) if decoder_hidden_list[k][1] is not None  else None   for k in range(beam_width)]\n",
    "            #update the list of h,c so that for the next input appropriate decoder state can be given as input\n",
    "            #using the sources of the input indices\n",
    "            \n",
    "        #if no <pad> comes in between and to consider incomplete sentences that reached max length\n",
    "        for j in range(beam_width):\n",
    "            score[:,j] = score[:,j]*(1/(i+2))                         #consider all top scores , normalize by length\n",
    "            final_score.append(score[:,j])\n",
    "            sent.append((i+1,j))\n",
    "            \n",
    "        indices = output.topk(beam_width , dim =1)[1]\n",
    "        indices_source = torch.floor((indices)*(1/output_size))\n",
    "        indices_char = (indices)%(output_size)\n",
    "        char_source = zip(indices_source ,indices_char)\n",
    "        indices_list.append(char_source)                          #appended at i+1 position\n",
    "            \n",
    "                \n",
    "        \n",
    "        \n",
    "        return indices_list , final_score , sent\n",
    "                \n",
    "    \n",
    "    #make sentence for a batch element            \n",
    "    \n",
    "    def sentence_maker(self,idx , bt , sent , indices_list ):\n",
    "        i, j = sent[idx]                    # i is used for length and j to get the char from possible top indices\n",
    "        sentence = []\n",
    "        for x in range(i , -1 ,-1):\n",
    "            indices = indices_list[x]       #get the char and source at x position in list\n",
    "            src , char = zip(*indices)      #unzip to get the char and its source\n",
    "            sent_idx = char[bt][int(j)]          \n",
    "            sent_src = src[bt][int(j)]           #get the char and source for this batch element\n",
    "            sentence.append(sent_idx)       \n",
    "            j = sent_src                    #now get the char at this char's source position\n",
    "            \n",
    "            indices_list[x] = zip(src,char) #zip it again to be used by other batch elements\n",
    "            \n",
    "        sentence = torch.FloatTensor(sentence)\n",
    "        sentence = sentence.flip(0)             #flip the sentence as list was made by backtracking\n",
    "        \n",
    "        return sentence                     #return sentence tensor\n",
    "    \n",
    "    \n",
    "    #main function \n",
    "     \n",
    "    def sentence_inference(self , batch_size,beam_width ,encoder_output, decoder_hidden):\n",
    "        beam_width = beam_width\n",
    "        indices_list, final_score, sent = self.beam_search_inference(batch_size,beam_width ,encoder_output ,decoder_hidden)\n",
    "        batch_sentence = [] \n",
    "        for bt in range(batch_size):\n",
    "            scr = [final_score[i][bt] for i in range(len(final_score))]  #for each element from batch make list of  \n",
    "            idx = scr.index(max(scr))                                    #final_scores of different sentence and get\n",
    "                                                                         #the max score\n",
    "            sentence = self.sentence_maker(idx , bt , sent , indices_list)    #get the sentence for current element of batch\n",
    "            batch_sentence.append(sentence)                              #make list of all sentences\n",
    "            \n",
    "        return batch_sentence                                            #return list of sentences tensor\n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "    def forward(self , seq , y=None , beam_width = 5 ,force_prob = 0.7):\n",
    "        self.force_prob = force_prob\n",
    "        self.beam_width = beam_width\n",
    "        batch_size = seq[0].size(0)\n",
    "        encoder_hidden = self.init_hidden(batch_size)\n",
    "        encoder_input = self.encoder_dropout(self.encoder_embedding_layer(seq))   #(seqlen , batch , embz)\n",
    "        encoder_output , encoder_hidden = self.encoder_LSTM(encoder_input ,encoder_hidden) \n",
    "        # (seqlen,batch,2*hiddensize) , (2*l , batch , hidden_size)\n",
    "            \n",
    "        encoder_hidden = self.cat_directions(encoder_hidden)    #(l , batch , 2*hidden_size)\n",
    "        if y is not None:\n",
    "            output = self.decoder_forward(batch_size ,encoder_output ,encoder_hidden ,self.force_prob, y=y )\n",
    "            #training phase ,here output is tuple \n",
    "            \n",
    "        else:\n",
    "            output = self.sentence_inference(batch_size,self.beam_width , encoder_output,encoder_hidden)\n",
    "            # inference phase , here output is list of all sentences\n",
    "            \n",
    "        return output\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(path, filename , file):\n",
    "    with open(f'{path}/{filename}' , 'wb') as f:\n",
    "        pickle.dump(file , f)\n",
    "        \n",
    "\n",
    "def norm_pretrained_embeddings(vecs , itos ,em_sz , padding_idx,vec_mean,vec_std):\n",
    "    #to make length of each vector equal and normalize them\n",
    "    emb = nn.Embedding(len(itos) , em_sz , padding_idx =padding_idx)\n",
    "    wgt = emb.weight.data\n",
    "    for i,w in enumerate(itos):\n",
    "        \n",
    "        wgt[i] = ((vecs[i] - vec_mean)/vec_std)      # vocab.vectors are used from index of char\n",
    "                    \n",
    "    emb.weight.requires_grad = False\n",
    "    return emb\n",
    "    \n",
    "def embedding_param(path, data_field, pre_trained_vector_type, embz_size=128, save_vocab=False, itos='itos',\n",
    "                    stoi='stoi'):\n",
    "    # to copy glove vectors from TEXT to pre_trained_vector(matrix and obtained after normalizing) which will\n",
    "    # be passed to model so that encoder_embeding can copy data from it\n",
    "    pre_trained = None\n",
    "    padding_idx = data_field.vocab.stoi['<pad>']\n",
    "    index_to_string , string_to_index = data_field.vocab.itos , data_field.vocab.stoi\n",
    "    if save_vocab:\n",
    "        vocab_path = os.path.join(path , \"vocab\")\n",
    "        os.makedirs(vocab_path, exist_ok=True)\n",
    "        save_pickle(vocab_path, f'{itos}.pk', index_to_string)\n",
    "        save_pickle(vocab_path ,f'{stoi}.pk' , string_to_index)\n",
    "    if pre_trained_vector_type:\n",
    "        vec_mean , vec_std = data_field.vocab.vectors.numpy().mean() , data_field.vocab.vectors.numpy().std()\n",
    "    print('pre_trained_vector_type mean = %s , pre_trained_vector_type std = %s' %(vec_mean ,vec_std)) \n",
    "    vector_wt_matrix = data_field.vocab.vectors\n",
    "    embz_size = vector_wt_matrix.size(1)\n",
    "    print('vector_wt_matrix size : \\n' ,(vector_wt_matrix.size()))\n",
    "    \n",
    "    pre_trained = norm_pretrained_embeddings(vector_wt_matrix ,index_to_string ,embz_size ,padding_idx,vec_mean,vec_std)\n",
    "    print('Normalizing... \\npre_trained_vector_mean = %s, pre_trained_vector_std=%s '\n",
    "          %(pre_trained.weight.data.numpy().mean() , pre_trained.weight.data.numpy().std()))\n",
    "    return pre_trained , embz_size ,padding_idx \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepanshu/anaconda3/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_trained_vector_type mean = 0.0026343681 , pre_trained_vector_type std = 0.43798193\n",
      "vector_wt_matrix size : \n",
      " torch.Size([41230, 200])\n",
      "Normalizing... \n",
      "pre_trained_vector_mean = 3.321553e-09, pre_trained_vector_std=0.99999994 \n"
     ]
    }
   ],
   "source": [
    "pre_trained_vector , embz_size , padding_idx = embedding_param(SampleDataPath, TEXT ,pre_trained_vector_type ,\n",
    "                                                              save_vocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "its = [next(train_iter_tuple.__iter__())[0]  for i in range(20)]\n",
    "max_tgt_len = int(np.percentile([its[o].size(0) for o in range(len(its))] , 99))\n",
    "max_tgt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(TEXT.vocab)\n",
    "hidden_size = 200\n",
    "output_size = len(TEXT.vocab)\n",
    "max_tgt_len = max_tgt_len\n",
    "beam_width = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seqloss(output , target):\n",
    "    sl,bs = target.size()\n",
    "    sl_in,bs_in,nc = output.size()           #nc is no of classes or words = output_size =len(TEXT.vocab)\n",
    "    if sl>sl_in: output = F.pad(output, (0,0,0,0,0,sl-sl_in))\n",
    "    output = output[:sl]\n",
    "    return F.cross_entropy(output.view(-1,nc), target.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    " model = Seq2Seq(input_size , embz_size , hidden_size, output_size ,batch_size ,max_tgt_len,pre_trained_vector,\n",
    "                pre_trained_vector_type, padding_idx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 ,    10] loss = 4.281\n",
      "[1 ,    20] loss = 3.962\n",
      "[1 ,    30] loss = 4.405\n",
      "[1 ,    40] loss = 3.817\n",
      "[1 ,    50] loss = 3.983\n",
      "[1 ,    60] loss = 4.462\n",
      "[1 ,    70] loss = 4.131\n",
      "[1 ,    80] loss = 4.664\n",
      "[1 ,    90] loss = 4.628\n",
      "[1 ,   100] loss = 4.521\n",
      "[1 ,   110] loss = 4.214\n",
      "[1 ,   120] loss = 4.328\n",
      "[1 ,   130] loss = 4.530\n",
      "[1 ,   140] loss = 4.392\n",
      "[1 ,   150] loss = 4.188\n",
      "[1 ,   160] loss = 4.166\n",
      "[1 ,   170] loss = 4.413\n",
      "[1 ,   180] loss = 4.330\n",
      "[1 ,   190] loss = 4.410\n",
      "[1 ,   200] loss = 4.515\n",
      "[1 ,   210] loss = 4.319\n",
      "[1 ,   220] loss = 4.306\n",
      "[1 ,   230] loss = 4.599\n",
      "[1 ,   240] loss = 4.130\n",
      "[1 ,   250] loss = 4.279\n",
      "[1 ,   260] loss = 4.680\n",
      "[1 ,   270] loss = 4.674\n",
      "[1 ,   280] loss = 4.569\n",
      "[1 ,   290] loss = 4.093\n",
      "[1 ,   300] loss = 4.179\n",
      "[1 ,   310] loss = 5.160\n",
      "[1 ,   320] loss = 4.474\n",
      "[1 ,   330] loss = 3.852\n",
      "[1 ,   340] loss = 3.955\n",
      "[1 ,   350] loss = 4.418\n",
      "[1 ,   360] loss = 4.518\n",
      "[1 ,   370] loss = 4.325\n",
      "[1 ,   380] loss = 4.027\n",
      "[1 ,   390] loss = 4.838\n",
      "[1 ,   400] loss = 4.452\n",
      "[1 ,   410] loss = 4.318\n",
      "[1 ,   420] loss = 3.957\n",
      "[1 ,   430] loss = 3.839\n",
      "[1 ,   440] loss = 3.778\n",
      "[1 ,   450] loss = 4.083\n",
      "[1 ,   460] loss = 4.381\n",
      "[1 ,   470] loss = 4.372\n",
      "[1 ,   480] loss = 4.231\n",
      "[1 ,   490] loss = 5.068\n",
      "[1 ,   500] loss = 4.393\n",
      "[1 ,   510] loss = 4.231\n",
      "[1 ,   520] loss = 4.349\n",
      "[1 ,   530] loss = 4.095\n",
      "[1 ,   540] loss = 4.220\n",
      "[1 ,   550] loss = 4.143\n",
      "[1 ,   560] loss = 4.262\n",
      "[1 ,   570] loss = 4.499\n",
      "[1 ,   580] loss = 4.447\n",
      "[1 ,   590] loss = 3.987\n",
      "[1 ,   600] loss = 4.212\n",
      "[1 ,   610] loss = 4.226\n",
      "[1 ,   620] loss = 4.226\n",
      "[1 ,   630] loss = 4.232\n",
      "[1 ,   640] loss = 4.027\n",
      "[1 ,   650] loss = 4.222\n",
      "[1 ,   660] loss = 4.277\n",
      "[1 ,   670] loss = 3.706\n",
      "[1 ,   680] loss = 4.210\n",
      "[1 ,   690] loss = 4.415\n",
      "[1 ,   700] loss = 4.681\n",
      "[1 ,   710] loss = 4.530\n",
      "[1 ,   720] loss = 3.919\n",
      "[1 ,   730] loss = 4.197\n",
      "[1 ,   740] loss = 4.038\n",
      "[1 ,   750] loss = 4.189\n",
      "[1 ,   760] loss = 4.002\n",
      "[1 ,   770] loss = 4.023\n",
      "[1 ,   780] loss = 4.343\n",
      "[1 ,   790] loss = 3.814\n",
      "[1 ,   800] loss = 4.088\n",
      "[1 ,   810] loss = 3.823\n",
      "[1 ,   820] loss = 5.014\n",
      "[1 ,   830] loss = 4.387\n",
      "[1 ,   840] loss = 4.922\n",
      "[1 ,   850] loss = 4.086\n",
      "[1 ,   860] loss = 4.309\n",
      "[1 ,   870] loss = 3.810\n",
      "[1 ,   880] loss = 3.828\n",
      "[1 ,   890] loss = 4.107\n",
      "[1 ,   900] loss = 4.387\n",
      "[1 ,   910] loss = 4.297\n",
      "[1 ,   920] loss = 4.750\n",
      "[1 ,   930] loss = 3.929\n",
      "[1 ,   940] loss = 3.953\n",
      "[1 ,   950] loss = 4.479\n",
      "[1 ,   960] loss = 4.013\n",
      "[1 ,   970] loss = 3.746\n",
      "[1 ,   980] loss = 4.093\n",
      "[1 ,   990] loss = 4.234\n",
      "[1 ,  1000] loss = 3.970\n",
      "[1 ,  1010] loss = 4.068\n",
      "[1 ,  1020] loss = 4.278\n",
      "[1 ,  1030] loss = 4.311\n",
      "[1 ,  1040] loss = 4.256\n",
      "[1 ,  1050] loss = 4.432\n",
      "[1 ,  1060] loss = 3.845\n",
      "[1 ,  1070] loss = 4.245\n",
      "[1 ,  1080] loss = 3.952\n",
      "[1 ,  1090] loss = 3.884\n",
      "[1 ,  1100] loss = 3.778\n",
      "[1 ,  1110] loss = 4.583\n",
      "[1 ,  1120] loss = 3.916\n",
      "[1 ,  1130] loss = 4.599\n",
      "[1 ,  1140] loss = 4.239\n",
      "[1 ,  1150] loss = 4.139\n",
      "[1 ,  1160] loss = 4.162\n",
      "[1 ,  1170] loss = 4.317\n",
      "[1 ,  1180] loss = 3.942\n",
      "[1 ,  1190] loss = 4.111\n",
      "[1 ,  1200] loss = 4.033\n",
      "[1 ,  1210] loss = 4.357\n",
      "[1 ,  1220] loss = 4.275\n",
      "[1 ,  1230] loss = 4.174\n",
      "[1 ,  1240] loss = 4.198\n",
      "[1 ,  1250] loss = 3.698\n",
      "[1 ,  1260] loss = 4.630\n",
      "[1 ,  1270] loss = 4.524\n",
      "[1 ,  1280] loss = 4.063\n",
      "[1 ,  1290] loss = 4.170\n",
      "[1 ,  1300] loss = 3.797\n",
      "[1 ,  1310] loss = 3.908\n",
      "[1 ,  1320] loss = 4.342\n",
      "[1 ,  1330] loss = 4.143\n",
      "[1 ,  1340] loss = 3.831\n",
      "[1 ,  1350] loss = 4.300\n",
      "[1 ,  1360] loss = 3.874\n",
      "[1 ,  1370] loss = 4.101\n",
      "[1 ,  1380] loss = 3.796\n",
      "[1 ,  1390] loss = 3.833\n",
      "[1 ,  1400] loss = 3.690\n",
      "[1 ,  1410] loss = 4.022\n",
      "[1 ,  1420] loss = 4.259\n",
      "[1 ,  1430] loss = 4.581\n",
      "[1 ,  1440] loss = 4.471\n",
      "[1 ,  1450] loss = 4.441\n",
      "[1 ,  1460] loss = 4.049\n",
      "[1 ,  1470] loss = 3.941\n",
      "[1 ,  1480] loss = 3.821\n",
      "[1 ,  1490] loss = 4.167\n",
      "[1 ,  1500] loss = 3.643\n",
      "[1 ,  1510] loss = 3.744\n",
      "[1 ,  1520] loss = 4.282\n",
      "[1 ,  1530] loss = 3.958\n",
      "[1 ,  1540] loss = 4.110\n",
      "[1 ,  1550] loss = 4.566\n",
      "[1 ,  1560] loss = 4.232\n",
      "[1 ,  1570] loss = 4.412\n",
      "[1 ,  1580] loss = 4.356\n",
      "[1 ,  1590] loss = 3.626\n",
      "[1 ,  1600] loss = 4.109\n",
      "[1 ,  1610] loss = 4.210\n",
      "[1 ,  1620] loss = 4.436\n",
      "[1 ,  1630] loss = 4.392\n",
      "[1 ,  1640] loss = 4.081\n",
      "[1 ,  1650] loss = 4.097\n",
      "[1 ,  1660] loss = 4.651\n",
      "[1 ,  1670] loss = 4.154\n",
      "[1 ,  1680] loss = 4.313\n",
      "[1 ,  1690] loss = 4.062\n",
      "[1 ,  1700] loss = 4.117\n",
      "[1 ,  1710] loss = 4.600\n",
      "[1 ,  1720] loss = 3.769\n",
      "[1 ,  1730] loss = 3.868\n",
      "[1 ,  1740] loss = 3.892\n",
      "[1 ,  1750] loss = 3.571\n",
      "[1 ,  1760] loss = 4.756\n",
      "[1 ,  1770] loss = 3.713\n",
      "[1 ,  1780] loss = 3.763\n",
      "[1 ,  1790] loss = 3.947\n",
      "[1 ,  1800] loss = 3.592\n",
      "[1 ,  1810] loss = 3.913\n",
      "[1 ,  1820] loss = 4.596\n",
      "[1 ,  1830] loss = 3.927\n",
      "[1 ,  1840] loss = 4.392\n",
      "[1 ,  1850] loss = 4.062\n",
      "[1 ,  1860] loss = 4.003\n",
      "[1 ,  1870] loss = 4.303\n",
      "[1 ,  1880] loss = 3.921\n",
      "[1 ,  1890] loss = 3.942\n",
      "[1 ,  1900] loss = 3.942\n",
      "[1 ,  1910] loss = 3.896\n",
      "[1 ,  1920] loss = 3.945\n",
      "[1 ,  1930] loss = 3.803\n",
      "[1 ,  1940] loss = 4.055\n",
      "[1 ,  1950] loss = 4.032\n",
      "[1 ,  1960] loss = 3.931\n",
      "[1 ,  1970] loss = 3.612\n",
      "[1 ,  1980] loss = 4.108\n",
      "[1 ,  1990] loss = 4.019\n",
      "[1 ,  2000] loss = 3.767\n",
      "[1 ,  2010] loss = 3.795\n",
      "[1 ,  2020] loss = 4.355\n",
      "[1 ,  2030] loss = 3.573\n",
      "[1 ,  2040] loss = 4.036\n",
      "[1 ,  2050] loss = 4.324\n",
      "[1 ,  2060] loss = 4.047\n",
      "[1 ,  2070] loss = 4.115\n",
      "[1 ,  2080] loss = 4.073\n",
      "[1 ,  2090] loss = 3.776\n",
      "[1 ,  2100] loss = 4.074\n",
      "[1 ,  2110] loss = 4.032\n",
      "[1 ,  2120] loss = 3.847\n",
      "[1 ,  2130] loss = 3.733\n",
      "[1 ,  2140] loss = 4.151\n",
      "[1 ,  2150] loss = 4.029\n",
      "[1 ,  2160] loss = 3.664\n",
      "[1 ,  2170] loss = 3.636\n",
      "[1 ,  2180] loss = 4.030\n",
      "[1 ,  2190] loss = 3.621\n",
      "[1 ,  2200] loss = 4.254\n",
      "[1 ,  2210] loss = 3.827\n",
      "[1 ,  2220] loss = 3.480\n",
      "[1 ,  2230] loss = 3.668\n",
      "[1 ,  2240] loss = 3.829\n",
      "[1 ,  2250] loss = 3.758\n",
      "[1 ,  2260] loss = 4.195\n",
      "[1 ,  2270] loss = 3.565\n",
      "[1 ,  2280] loss = 4.032\n",
      "[1 ,  2290] loss = 4.215\n",
      "[1 ,  2300] loss = 4.122\n",
      "[1 ,  2310] loss = 4.049\n",
      "[1 ,  2320] loss = 4.344\n",
      "[1 ,  2330] loss = 3.751\n",
      "[1 ,  2340] loss = 3.712\n",
      "[1 ,  2350] loss = 3.752\n",
      "[1 ,  2360] loss = 3.804\n",
      "[1 ,  2370] loss = 3.985\n",
      "[1 ,  2380] loss = 4.311\n",
      "[1 ,  2390] loss = 4.113\n",
      "[1 ,  2400] loss = 3.862\n",
      "[1 ,  2410] loss = 3.979\n",
      "[1 ,  2420] loss = 3.667\n",
      "[1 ,  2430] loss = 4.142\n",
      "[1 ,  2440] loss = 3.617\n",
      "[1 ,  2450] loss = 3.812\n",
      "[1 ,  2460] loss = 3.738\n",
      "[1 ,  2470] loss = 3.668\n",
      "[1 ,  2480] loss = 3.579\n",
      "[1 ,  2490] loss = 3.627\n",
      "[1 ,  2500] loss = 3.670\n",
      "[2 ,    10] loss = 4.307\n",
      "[2 ,    20] loss = 4.520\n",
      "[2 ,    30] loss = 4.193\n",
      "[2 ,    40] loss = 4.164\n",
      "[2 ,    50] loss = 4.225\n",
      "[2 ,    60] loss = 4.692\n",
      "[2 ,    70] loss = 3.847\n",
      "[2 ,    80] loss = 4.172\n",
      "[2 ,    90] loss = 4.082\n",
      "[2 ,   100] loss = 3.782\n",
      "[2 ,   110] loss = 4.572\n",
      "[2 ,   120] loss = 3.943\n",
      "[2 ,   130] loss = 4.249\n",
      "[2 ,   140] loss = 4.400\n",
      "[2 ,   150] loss = 4.345\n",
      "[2 ,   160] loss = 3.775\n",
      "[2 ,   170] loss = 3.748\n",
      "[2 ,   180] loss = 3.736\n",
      "[2 ,   190] loss = 4.104\n",
      "[2 ,   200] loss = 3.732\n",
      "[2 ,   210] loss = 4.044\n",
      "[2 ,   220] loss = 4.315\n",
      "[2 ,   230] loss = 3.803\n",
      "[2 ,   240] loss = 4.210\n",
      "[2 ,   250] loss = 4.808\n",
      "[2 ,   260] loss = 4.443\n",
      "[2 ,   270] loss = 4.128\n",
      "[2 ,   280] loss = 4.111\n",
      "[2 ,   290] loss = 4.294\n",
      "[2 ,   300] loss = 3.845\n",
      "[2 ,   310] loss = 3.941\n",
      "[2 ,   320] loss = 3.835\n",
      "[2 ,   330] loss = 3.918\n",
      "[2 ,   340] loss = 4.545\n",
      "[2 ,   350] loss = 4.289\n",
      "[2 ,   360] loss = 4.258\n",
      "[2 ,   370] loss = 4.461\n",
      "[2 ,   380] loss = 3.954\n",
      "[2 ,   390] loss = 4.005\n",
      "[2 ,   400] loss = 4.505\n",
      "[2 ,   410] loss = 3.965\n",
      "[2 ,   420] loss = 4.672\n",
      "[2 ,   430] loss = 4.135\n",
      "[2 ,   440] loss = 4.530\n",
      "[2 ,   450] loss = 4.268\n",
      "[2 ,   460] loss = 4.312\n",
      "[2 ,   470] loss = 3.792\n",
      "[2 ,   480] loss = 3.838\n",
      "[2 ,   490] loss = 4.118\n",
      "[2 ,   500] loss = 3.882\n",
      "[2 ,   510] loss = 3.678\n",
      "[2 ,   520] loss = 4.065\n",
      "[2 ,   530] loss = 3.920\n",
      "[2 ,   540] loss = 3.702\n",
      "[2 ,   550] loss = 3.419\n",
      "[2 ,   560] loss = 4.442\n",
      "[2 ,   570] loss = 3.701\n",
      "[2 ,   580] loss = 4.185\n",
      "[2 ,   590] loss = 3.864\n",
      "[2 ,   600] loss = 3.860\n",
      "[2 ,   610] loss = 4.090\n",
      "[2 ,   620] loss = 4.202\n",
      "[2 ,   630] loss = 3.415\n",
      "[2 ,   640] loss = 4.084\n",
      "[2 ,   650] loss = 4.274\n",
      "[2 ,   660] loss = 4.027\n",
      "[2 ,   670] loss = 3.834\n",
      "[2 ,   680] loss = 3.638\n",
      "[2 ,   690] loss = 3.997\n",
      "[2 ,   700] loss = 4.232\n",
      "[2 ,   710] loss = 3.373\n",
      "[2 ,   720] loss = 4.274\n",
      "[2 ,   730] loss = 4.228\n",
      "[2 ,   740] loss = 3.971\n",
      "[2 ,   750] loss = 4.227\n",
      "[2 ,   760] loss = 3.800\n",
      "[2 ,   770] loss = 3.714\n",
      "[2 ,   780] loss = 3.850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 ,   790] loss = 4.047\n",
      "[2 ,   800] loss = 3.680\n",
      "[2 ,   810] loss = 3.684\n",
      "[2 ,   820] loss = 3.827\n",
      "[2 ,   830] loss = 3.922\n",
      "[2 ,   840] loss = 4.875\n",
      "[2 ,   850] loss = 4.366\n",
      "[2 ,   860] loss = 3.949\n",
      "[2 ,   870] loss = 4.362\n",
      "[2 ,   880] loss = 4.396\n",
      "[2 ,   890] loss = 3.759\n",
      "[2 ,   900] loss = 3.848\n",
      "[2 ,   910] loss = 4.662\n",
      "[2 ,   920] loss = 4.113\n",
      "[2 ,   930] loss = 3.666\n",
      "[2 ,   940] loss = 4.683\n",
      "[2 ,   950] loss = 3.938\n",
      "[2 ,   960] loss = 3.901\n",
      "[2 ,   970] loss = 4.147\n",
      "[2 ,   980] loss = 3.812\n",
      "[2 ,   990] loss = 4.109\n",
      "[2 ,  1000] loss = 4.081\n",
      "[2 ,  1010] loss = 4.957\n",
      "[2 ,  1020] loss = 4.193\n",
      "[2 ,  1030] loss = 3.954\n",
      "[2 ,  1040] loss = 4.400\n",
      "[2 ,  1050] loss = 4.019\n",
      "[2 ,  1060] loss = 4.085\n",
      "[2 ,  1070] loss = 4.417\n",
      "[2 ,  1080] loss = 4.643\n",
      "[2 ,  1090] loss = 3.686\n",
      "[2 ,  1100] loss = 4.119\n",
      "[2 ,  1110] loss = 3.505\n",
      "[2 ,  1120] loss = 3.443\n",
      "[2 ,  1130] loss = 4.293\n",
      "[2 ,  1140] loss = 4.882\n",
      "[2 ,  1150] loss = 4.155\n",
      "[2 ,  1160] loss = 4.001\n",
      "[2 ,  1170] loss = 3.821\n",
      "[2 ,  1180] loss = 4.807\n",
      "[2 ,  1190] loss = 4.097\n",
      "[2 ,  1200] loss = 4.012\n",
      "[2 ,  1210] loss = 4.219\n",
      "[2 ,  1220] loss = 4.054\n",
      "[2 ,  1230] loss = 4.050\n",
      "[2 ,  1240] loss = 4.198\n",
      "[2 ,  1250] loss = 4.359\n",
      "[2 ,  1260] loss = 4.914\n",
      "[2 ,  1270] loss = 4.453\n",
      "[2 ,  1280] loss = 4.127\n",
      "[2 ,  1290] loss = 3.426\n",
      "[2 ,  1300] loss = 4.091\n",
      "[2 ,  1310] loss = 4.307\n",
      "[2 ,  1320] loss = 4.338\n",
      "[2 ,  1330] loss = 4.292\n",
      "[2 ,  1340] loss = 3.715\n",
      "[2 ,  1350] loss = 3.625\n",
      "[2 ,  1360] loss = 4.532\n",
      "[2 ,  1370] loss = 3.763\n",
      "[2 ,  1380] loss = 4.585\n",
      "[2 ,  1390] loss = 4.102\n",
      "[2 ,  1400] loss = 4.207\n",
      "[2 ,  1410] loss = 4.122\n",
      "[2 ,  1420] loss = 3.787\n",
      "[2 ,  1430] loss = 3.882\n",
      "[2 ,  1440] loss = 4.192\n",
      "[2 ,  1450] loss = 4.159\n",
      "[2 ,  1460] loss = 4.230\n",
      "[2 ,  1470] loss = 4.695\n",
      "[2 ,  1480] loss = 4.365\n",
      "[2 ,  1490] loss = 4.079\n",
      "[2 ,  1500] loss = 3.876\n",
      "[2 ,  1510] loss = 3.293\n",
      "[2 ,  1520] loss = 3.604\n",
      "[2 ,  1530] loss = 3.722\n",
      "[2 ,  1540] loss = 4.296\n",
      "[2 ,  1550] loss = 4.020\n",
      "[2 ,  1560] loss = 3.821\n",
      "[2 ,  1570] loss = 4.416\n",
      "[2 ,  1580] loss = 4.602\n",
      "[2 ,  1590] loss = 4.054\n",
      "[2 ,  1600] loss = 4.574\n",
      "[2 ,  1610] loss = 3.690\n",
      "[2 ,  1620] loss = 4.072\n",
      "[2 ,  1630] loss = 4.404\n",
      "[2 ,  1640] loss = 4.131\n",
      "[2 ,  1650] loss = 3.880\n",
      "[2 ,  1660] loss = 3.821\n",
      "[2 ,  1670] loss = 3.376\n",
      "[2 ,  1680] loss = 4.086\n",
      "[2 ,  1690] loss = 3.788\n",
      "[2 ,  1700] loss = 4.414\n",
      "[2 ,  1710] loss = 3.780\n",
      "[2 ,  1720] loss = 3.968\n",
      "[2 ,  1730] loss = 4.064\n",
      "[2 ,  1740] loss = 4.032\n",
      "[2 ,  1750] loss = 4.248\n",
      "[2 ,  1760] loss = 4.461\n",
      "[2 ,  1770] loss = 4.034\n",
      "[2 ,  1780] loss = 3.986\n",
      "[2 ,  1790] loss = 3.827\n",
      "[2 ,  1800] loss = 3.878\n",
      "[2 ,  1810] loss = 3.917\n",
      "[2 ,  1820] loss = 3.769\n",
      "[2 ,  1830] loss = 4.205\n",
      "[2 ,  1840] loss = 3.963\n",
      "[2 ,  1850] loss = 3.924\n",
      "[2 ,  1860] loss = 3.434\n",
      "[2 ,  1870] loss = 3.880\n",
      "[2 ,  1880] loss = 4.119\n",
      "[2 ,  1890] loss = 3.976\n",
      "[2 ,  1900] loss = 3.813\n",
      "[2 ,  1910] loss = 3.698\n",
      "[2 ,  1920] loss = 3.818\n",
      "[2 ,  1930] loss = 3.841\n",
      "[2 ,  1940] loss = 4.016\n",
      "[2 ,  1950] loss = 3.796\n",
      "[2 ,  1960] loss = 4.133\n",
      "[2 ,  1970] loss = 4.506\n",
      "[2 ,  1980] loss = 4.030\n",
      "[2 ,  1990] loss = 4.114\n",
      "[2 ,  2000] loss = 4.403\n",
      "[2 ,  2010] loss = 4.529\n",
      "[2 ,  2020] loss = 4.089\n",
      "[2 ,  2030] loss = 4.019\n",
      "[2 ,  2040] loss = 4.156\n",
      "[2 ,  2050] loss = 3.781\n",
      "[2 ,  2060] loss = 3.651\n",
      "[2 ,  2070] loss = 4.333\n",
      "[2 ,  2080] loss = 3.609\n",
      "[2 ,  2090] loss = 4.222\n",
      "[2 ,  2100] loss = 4.211\n",
      "[2 ,  2110] loss = 4.185\n",
      "[2 ,  2120] loss = 4.371\n",
      "[2 ,  2130] loss = 3.560\n",
      "[2 ,  2140] loss = 4.048\n",
      "[2 ,  2150] loss = 3.686\n",
      "[2 ,  2160] loss = 3.781\n",
      "[2 ,  2170] loss = 3.689\n",
      "[2 ,  2180] loss = 4.617\n",
      "[2 ,  2190] loss = 4.407\n",
      "[2 ,  2200] loss = 4.698\n",
      "[2 ,  2210] loss = 4.408\n",
      "[2 ,  2220] loss = 3.481\n",
      "[2 ,  2230] loss = 3.853\n",
      "[2 ,  2240] loss = 3.752\n",
      "[2 ,  2250] loss = 3.931\n",
      "[2 ,  2260] loss = 4.009\n",
      "[2 ,  2270] loss = 3.990\n",
      "[2 ,  2280] loss = 3.882\n",
      "[2 ,  2290] loss = 3.950\n",
      "[2 ,  2300] loss = 3.942\n",
      "[2 ,  2310] loss = 3.773\n",
      "[2 ,  2320] loss = 3.840\n",
      "[2 ,  2330] loss = 3.730\n",
      "[2 ,  2340] loss = 4.867\n",
      "[2 ,  2350] loss = 3.744\n",
      "[2 ,  2360] loss = 3.610\n",
      "[2 ,  2370] loss = 4.161\n",
      "[2 ,  2380] loss = 3.508\n",
      "[2 ,  2390] loss = 3.939\n",
      "[2 ,  2400] loss = 3.928\n",
      "[2 ,  2410] loss = 3.841\n",
      "[2 ,  2420] loss = 4.472\n",
      "[2 ,  2430] loss = 4.108\n",
      "[2 ,  2440] loss = 4.454\n",
      "[2 ,  2450] loss = 4.158\n",
      "[2 ,  2460] loss = 3.865\n",
      "[2 ,  2470] loss = 4.000\n",
      "[2 ,  2480] loss = 4.274\n",
      "[2 ,  2490] loss = 3.996\n",
      "[2 ,  2500] loss = 4.087\n",
      "[3 ,    10] loss = 3.775\n",
      "[3 ,    20] loss = 3.829\n",
      "[3 ,    30] loss = 3.510\n",
      "[3 ,    40] loss = 3.964\n",
      "[3 ,    50] loss = 3.489\n",
      "[3 ,    60] loss = 4.020\n",
      "[3 ,    70] loss = 3.525\n",
      "[3 ,    80] loss = 3.973\n",
      "[3 ,    90] loss = 3.602\n",
      "[3 ,   100] loss = 3.875\n",
      "[3 ,   110] loss = 4.147\n",
      "[3 ,   120] loss = 3.572\n",
      "[3 ,   130] loss = 4.166\n",
      "[3 ,   140] loss = 4.354\n",
      "[3 ,   150] loss = 3.710\n",
      "[3 ,   160] loss = 3.858\n",
      "[3 ,   170] loss = 3.655\n",
      "[3 ,   180] loss = 3.755\n",
      "[3 ,   190] loss = 3.741\n",
      "[3 ,   200] loss = 4.460\n",
      "[3 ,   210] loss = 4.211\n",
      "[3 ,   220] loss = 3.733\n",
      "[3 ,   230] loss = 4.024\n",
      "[3 ,   240] loss = 4.109\n",
      "[3 ,   250] loss = 4.455\n",
      "[3 ,   260] loss = 4.046\n",
      "[3 ,   270] loss = 4.268\n",
      "[3 ,   280] loss = 3.968\n",
      "[3 ,   290] loss = 4.515\n",
      "[3 ,   300] loss = 4.268\n",
      "[3 ,   310] loss = 4.184\n",
      "[3 ,   320] loss = 3.578\n",
      "[3 ,   330] loss = 4.333\n",
      "[3 ,   340] loss = 4.031\n",
      "[3 ,   350] loss = 3.711\n",
      "[3 ,   360] loss = 3.847\n",
      "[3 ,   370] loss = 3.796\n",
      "[3 ,   380] loss = 3.957\n",
      "[3 ,   390] loss = 3.602\n",
      "[3 ,   400] loss = 4.467\n",
      "[3 ,   410] loss = 3.722\n",
      "[3 ,   420] loss = 4.258\n",
      "[3 ,   430] loss = 3.762\n",
      "[3 ,   440] loss = 3.719\n",
      "[3 ,   450] loss = 3.599\n",
      "[3 ,   460] loss = 3.907\n",
      "[3 ,   470] loss = 4.634\n",
      "[3 ,   480] loss = 4.007\n",
      "[3 ,   490] loss = 4.358\n",
      "[3 ,   500] loss = 4.116\n",
      "[3 ,   510] loss = 4.006\n",
      "[3 ,   520] loss = 3.499\n",
      "[3 ,   530] loss = 3.436\n",
      "[3 ,   540] loss = 3.686\n",
      "[3 ,   550] loss = 4.275\n",
      "[3 ,   560] loss = 3.705\n",
      "[3 ,   570] loss = 3.699\n",
      "[3 ,   580] loss = 3.801\n",
      "[3 ,   590] loss = 3.760\n",
      "[3 ,   600] loss = 3.788\n",
      "[3 ,   610] loss = 4.059\n",
      "[3 ,   620] loss = 3.986\n",
      "[3 ,   630] loss = 3.511\n",
      "[3 ,   640] loss = 3.469\n",
      "[3 ,   650] loss = 3.721\n",
      "[3 ,   660] loss = 4.577\n",
      "[3 ,   670] loss = 3.968\n",
      "[3 ,   680] loss = 4.119\n",
      "[3 ,   690] loss = 3.903\n",
      "[3 ,   700] loss = 4.212\n",
      "[3 ,   710] loss = 3.427\n",
      "[3 ,   720] loss = 4.526\n",
      "[3 ,   730] loss = 4.014\n",
      "[3 ,   740] loss = 3.671\n",
      "[3 ,   750] loss = 4.084\n",
      "[3 ,   760] loss = 4.149\n",
      "[3 ,   770] loss = 3.550\n",
      "[3 ,   780] loss = 4.157\n",
      "[3 ,   790] loss = 4.219\n",
      "[3 ,   800] loss = 3.854\n",
      "[3 ,   810] loss = 3.809\n",
      "[3 ,   820] loss = 3.993\n",
      "[3 ,   830] loss = 4.126\n",
      "[3 ,   840] loss = 3.598\n",
      "[3 ,   850] loss = 3.581\n",
      "[3 ,   860] loss = 3.619\n",
      "[3 ,   870] loss = 3.592\n",
      "[3 ,   880] loss = 3.791\n",
      "[3 ,   890] loss = 4.204\n",
      "[3 ,   900] loss = 3.966\n",
      "[3 ,   910] loss = 4.118\n",
      "[3 ,   920] loss = 3.779\n",
      "[3 ,   930] loss = 4.278\n",
      "[3 ,   940] loss = 3.738\n",
      "[3 ,   950] loss = 3.756\n",
      "[3 ,   960] loss = 3.852\n",
      "[3 ,   970] loss = 3.980\n",
      "[3 ,   980] loss = 4.046\n",
      "[3 ,   990] loss = 3.849\n",
      "[3 ,  1000] loss = 4.118\n",
      "[3 ,  1010] loss = 3.989\n",
      "[3 ,  1020] loss = 4.410\n",
      "[3 ,  1030] loss = 3.846\n",
      "[3 ,  1040] loss = 3.509\n",
      "[3 ,  1050] loss = 4.213\n",
      "[3 ,  1060] loss = 4.367\n",
      "[3 ,  1070] loss = 3.858\n",
      "[3 ,  1080] loss = 3.973\n",
      "[3 ,  1090] loss = 4.458\n",
      "[3 ,  1100] loss = 3.825\n",
      "[3 ,  1110] loss = 4.372\n",
      "[3 ,  1120] loss = 3.596\n",
      "[3 ,  1130] loss = 4.043\n",
      "[3 ,  1140] loss = 4.067\n",
      "[3 ,  1150] loss = 3.697\n",
      "[3 ,  1160] loss = 3.833\n",
      "[3 ,  1170] loss = 3.657\n",
      "[3 ,  1180] loss = 3.950\n",
      "[3 ,  1190] loss = 3.297\n",
      "[3 ,  1200] loss = 3.309\n",
      "[3 ,  1210] loss = 3.754\n",
      "[3 ,  1220] loss = 4.341\n",
      "[3 ,  1230] loss = 5.005\n",
      "[3 ,  1240] loss = 3.815\n",
      "[3 ,  1250] loss = 3.809\n",
      "[3 ,  1260] loss = 4.052\n",
      "[3 ,  1270] loss = 4.009\n",
      "[3 ,  1280] loss = 4.164\n",
      "[3 ,  1290] loss = 4.081\n",
      "[3 ,  1300] loss = 3.688\n",
      "[3 ,  1310] loss = 3.926\n",
      "[3 ,  1320] loss = 3.632\n",
      "[3 ,  1330] loss = 4.385\n",
      "[3 ,  1340] loss = 3.492\n",
      "[3 ,  1350] loss = 3.817\n",
      "[3 ,  1360] loss = 3.848\n",
      "[3 ,  1370] loss = 4.247\n",
      "[3 ,  1380] loss = 3.792\n",
      "[3 ,  1390] loss = 4.203\n",
      "[3 ,  1400] loss = 4.334\n",
      "[3 ,  1410] loss = 3.603\n",
      "[3 ,  1420] loss = 3.472\n",
      "[3 ,  1430] loss = 4.117\n",
      "[3 ,  1440] loss = 3.955\n",
      "[3 ,  1450] loss = 3.854\n",
      "[3 ,  1460] loss = 4.028\n",
      "[3 ,  1470] loss = 3.447\n",
      "[3 ,  1480] loss = 3.650\n",
      "[3 ,  1490] loss = 4.293\n",
      "[3 ,  1500] loss = 3.825\n",
      "[3 ,  1510] loss = 3.525\n",
      "[3 ,  1520] loss = 3.678\n",
      "[3 ,  1530] loss = 3.806\n",
      "[3 ,  1540] loss = 5.021\n",
      "[3 ,  1550] loss = 4.119\n",
      "[3 ,  1560] loss = 4.182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 ,  1570] loss = 3.550\n",
      "[3 ,  1580] loss = 3.567\n",
      "[3 ,  1590] loss = 4.345\n",
      "[3 ,  1600] loss = 4.325\n",
      "[3 ,  1610] loss = 3.880\n",
      "[3 ,  1620] loss = 4.005\n",
      "[3 ,  1630] loss = 3.329\n",
      "[3 ,  1640] loss = 3.483\n",
      "[3 ,  1650] loss = 4.078\n",
      "[3 ,  1660] loss = 3.792\n",
      "[3 ,  1670] loss = 3.837\n",
      "[3 ,  1680] loss = 3.949\n",
      "[3 ,  1690] loss = 4.085\n",
      "[3 ,  1700] loss = 3.964\n",
      "[3 ,  1710] loss = 4.950\n",
      "[3 ,  1720] loss = 3.425\n",
      "[3 ,  1730] loss = 3.680\n",
      "[3 ,  1740] loss = 3.992\n",
      "[3 ,  1750] loss = 3.687\n",
      "[3 ,  1760] loss = 4.305\n",
      "[3 ,  1770] loss = 3.820\n",
      "[3 ,  1780] loss = 3.955\n",
      "[3 ,  1790] loss = 3.504\n",
      "[3 ,  1800] loss = 3.255\n",
      "[3 ,  1810] loss = 3.303\n",
      "[3 ,  1820] loss = 4.212\n",
      "[3 ,  1830] loss = 4.115\n",
      "[3 ,  1840] loss = 4.470\n",
      "[3 ,  1850] loss = 3.629\n",
      "[3 ,  1860] loss = 3.620\n",
      "[3 ,  1870] loss = 4.508\n",
      "[3 ,  1880] loss = 3.740\n",
      "[3 ,  1890] loss = 4.784\n",
      "[3 ,  1900] loss = 4.049\n",
      "[3 ,  1910] loss = 3.361\n",
      "[3 ,  1920] loss = 3.810\n",
      "[3 ,  1930] loss = 4.435\n",
      "[3 ,  1940] loss = 3.493\n",
      "[3 ,  1950] loss = 4.179\n",
      "[3 ,  1960] loss = 3.753\n",
      "[3 ,  1970] loss = 3.938\n",
      "[3 ,  1980] loss = 4.138\n",
      "[3 ,  1990] loss = 3.824\n",
      "[3 ,  2000] loss = 4.386\n",
      "[3 ,  2010] loss = 3.661\n",
      "[3 ,  2020] loss = 4.135\n",
      "[3 ,  2030] loss = 3.793\n",
      "[3 ,  2040] loss = 3.832\n",
      "[3 ,  2050] loss = 4.180\n",
      "[3 ,  2060] loss = 3.970\n",
      "[3 ,  2070] loss = 3.989\n",
      "[3 ,  2080] loss = 3.714\n",
      "[3 ,  2090] loss = 4.219\n",
      "[3 ,  2100] loss = 3.919\n",
      "[3 ,  2110] loss = 4.045\n",
      "[3 ,  2120] loss = 4.795\n",
      "[3 ,  2130] loss = 3.913\n",
      "[3 ,  2140] loss = 4.432\n",
      "[3 ,  2150] loss = 4.181\n",
      "[3 ,  2160] loss = 4.160\n",
      "[3 ,  2170] loss = 4.015\n",
      "[3 ,  2180] loss = 3.944\n",
      "[3 ,  2190] loss = 3.408\n",
      "[3 ,  2200] loss = 3.555\n",
      "[3 ,  2210] loss = 4.085\n",
      "[3 ,  2220] loss = 3.715\n",
      "[3 ,  2230] loss = 4.098\n",
      "[3 ,  2240] loss = 3.957\n",
      "[3 ,  2250] loss = 3.209\n",
      "[3 ,  2260] loss = 4.050\n",
      "[3 ,  2270] loss = 3.740\n",
      "[3 ,  2280] loss = 4.633\n",
      "[3 ,  2290] loss = 4.060\n",
      "[3 ,  2300] loss = 3.740\n",
      "[3 ,  2310] loss = 4.092\n",
      "[3 ,  2320] loss = 3.679\n",
      "[3 ,  2330] loss = 4.604\n",
      "[3 ,  2340] loss = 3.994\n",
      "[3 ,  2350] loss = 3.837\n",
      "[3 ,  2360] loss = 4.074\n",
      "[3 ,  2370] loss = 3.762\n",
      "[3 ,  2380] loss = 3.365\n",
      "[3 ,  2390] loss = 3.509\n",
      "[3 ,  2400] loss = 4.170\n",
      "[3 ,  2410] loss = 3.982\n",
      "[3 ,  2420] loss = 4.470\n",
      "[3 ,  2430] loss = 3.451\n",
      "[3 ,  2440] loss = 3.955\n",
      "[3 ,  2450] loss = 4.371\n",
      "[3 ,  2460] loss = 4.406\n",
      "[3 ,  2470] loss = 3.532\n",
      "[3 ,  2480] loss = 4.619\n",
      "[3 ,  2490] loss = 3.926\n",
      "[3 ,  2500] loss = 4.235\n",
      "[4 ,    10] loss = 3.202\n",
      "[4 ,    20] loss = 3.655\n",
      "[4 ,    30] loss = 4.325\n",
      "[4 ,    40] loss = 3.426\n",
      "[4 ,    50] loss = 3.469\n",
      "[4 ,    60] loss = 3.421\n",
      "[4 ,    70] loss = 3.574\n",
      "[4 ,    80] loss = 3.816\n",
      "[4 ,    90] loss = 2.682\n",
      "[4 ,   100] loss = 2.728\n",
      "[4 ,   110] loss = 3.399\n",
      "[4 ,   120] loss = 3.033\n",
      "[4 ,   130] loss = 3.281\n",
      "[4 ,   140] loss = 3.818\n",
      "[4 ,   150] loss = 3.137\n",
      "[4 ,   160] loss = 3.492\n",
      "[4 ,   170] loss = 3.035\n",
      "[4 ,   180] loss = 3.241\n",
      "[4 ,   190] loss = 3.307\n",
      "[4 ,   200] loss = 3.341\n",
      "[4 ,   210] loss = 3.635\n",
      "[4 ,   220] loss = 3.251\n",
      "[4 ,   230] loss = 3.331\n",
      "[4 ,   240] loss = 3.228\n",
      "[4 ,   250] loss = 2.928\n",
      "[4 ,   260] loss = 3.626\n",
      "[4 ,   270] loss = 2.979\n",
      "[4 ,   280] loss = 3.110\n",
      "[4 ,   290] loss = 3.457\n",
      "[4 ,   300] loss = 3.266\n",
      "[4 ,   310] loss = 3.575\n",
      "[4 ,   320] loss = 3.114\n",
      "[4 ,   330] loss = 3.214\n",
      "[4 ,   340] loss = 3.214\n",
      "[4 ,   350] loss = 3.475\n",
      "[4 ,   360] loss = 3.308\n",
      "[4 ,   370] loss = 3.537\n",
      "[4 ,   380] loss = 2.930\n",
      "[4 ,   390] loss = 3.406\n",
      "[4 ,   400] loss = 3.360\n",
      "[4 ,   410] loss = 3.599\n",
      "[4 ,   420] loss = 3.017\n",
      "[4 ,   430] loss = 3.606\n",
      "[4 ,   440] loss = 3.393\n",
      "[4 ,   450] loss = 3.341\n",
      "[4 ,   460] loss = 3.004\n",
      "[4 ,   470] loss = 3.242\n",
      "[4 ,   480] loss = 3.640\n",
      "[4 ,   490] loss = 3.783\n",
      "[4 ,   500] loss = 3.608\n",
      "[4 ,   510] loss = 3.337\n",
      "[4 ,   520] loss = 3.222\n",
      "[4 ,   530] loss = 2.898\n",
      "[4 ,   540] loss = 3.089\n",
      "[4 ,   550] loss = 3.803\n",
      "[4 ,   560] loss = 3.546\n",
      "[4 ,   570] loss = 3.581\n",
      "[4 ,   580] loss = 3.330\n",
      "[4 ,   590] loss = 3.117\n",
      "[4 ,   600] loss = 3.709\n",
      "[4 ,   610] loss = 3.779\n",
      "[4 ,   620] loss = 2.966\n",
      "[4 ,   630] loss = 3.832\n",
      "[4 ,   640] loss = 3.477\n",
      "[4 ,   650] loss = 3.334\n",
      "[4 ,   660] loss = 3.558\n",
      "[4 ,   670] loss = 3.538\n",
      "[4 ,   680] loss = 3.515\n",
      "[4 ,   690] loss = 3.569\n",
      "[4 ,   700] loss = 3.208\n",
      "[4 ,   710] loss = 3.873\n",
      "[4 ,   720] loss = 3.104\n",
      "[4 ,   730] loss = 3.383\n",
      "[4 ,   740] loss = 3.164\n",
      "[4 ,   750] loss = 3.948\n",
      "[4 ,   760] loss = 3.738\n",
      "[4 ,   770] loss = 3.226\n",
      "[4 ,   780] loss = 3.524\n",
      "[4 ,   790] loss = 4.350\n",
      "[4 ,   800] loss = 3.513\n",
      "[4 ,   810] loss = 3.276\n",
      "[4 ,   820] loss = 3.123\n",
      "[4 ,   830] loss = 3.106\n",
      "[4 ,   840] loss = 3.624\n",
      "[4 ,   850] loss = 3.288\n",
      "[4 ,   860] loss = 3.112\n",
      "[4 ,   870] loss = 3.122\n",
      "[4 ,   880] loss = 3.673\n",
      "[4 ,   890] loss = 4.148\n",
      "[4 ,   900] loss = 3.617\n",
      "[4 ,   910] loss = 3.411\n",
      "[4 ,   920] loss = 3.350\n",
      "[4 ,   930] loss = 3.716\n",
      "[4 ,   940] loss = 3.368\n",
      "[4 ,   950] loss = 3.749\n",
      "[4 ,   960] loss = 3.851\n",
      "[4 ,   970] loss = 3.301\n",
      "[4 ,   980] loss = 3.273\n",
      "[4 ,   990] loss = 2.967\n",
      "[4 ,  1000] loss = 3.421\n",
      "[4 ,  1010] loss = 2.996\n",
      "[4 ,  1020] loss = 3.396\n",
      "[4 ,  1030] loss = 3.425\n",
      "[4 ,  1040] loss = 3.906\n",
      "[4 ,  1050] loss = 3.114\n",
      "[4 ,  1060] loss = 3.570\n",
      "[4 ,  1070] loss = 3.270\n",
      "[4 ,  1080] loss = 3.360\n",
      "[4 ,  1090] loss = 3.244\n",
      "[4 ,  1100] loss = 3.075\n",
      "[4 ,  1110] loss = 3.370\n",
      "[4 ,  1120] loss = 3.201\n",
      "[4 ,  1130] loss = 3.047\n",
      "[4 ,  1140] loss = 3.427\n",
      "[4 ,  1150] loss = 3.225\n",
      "[4 ,  1160] loss = 3.696\n",
      "[4 ,  1170] loss = 3.033\n",
      "[4 ,  1180] loss = 3.283\n",
      "[4 ,  1190] loss = 3.579\n",
      "[4 ,  1200] loss = 3.248\n",
      "[4 ,  1210] loss = 3.471\n",
      "[4 ,  1220] loss = 3.485\n",
      "[4 ,  1230] loss = 3.639\n",
      "[4 ,  1240] loss = 3.615\n",
      "[4 ,  1250] loss = 3.399\n",
      "[4 ,  1260] loss = 3.255\n",
      "[4 ,  1270] loss = 3.182\n",
      "[4 ,  1280] loss = 3.251\n",
      "[4 ,  1290] loss = 3.230\n",
      "[4 ,  1300] loss = 3.302\n",
      "[4 ,  1310] loss = 3.525\n",
      "[4 ,  1320] loss = 3.237\n",
      "[4 ,  1330] loss = 3.832\n",
      "[4 ,  1340] loss = 3.782\n",
      "[4 ,  1350] loss = 3.822\n",
      "[4 ,  1360] loss = 3.139\n",
      "[4 ,  1370] loss = 3.462\n",
      "[4 ,  1380] loss = 3.142\n",
      "[4 ,  1390] loss = 3.233\n",
      "[4 ,  1400] loss = 3.385\n",
      "[4 ,  1410] loss = 3.402\n",
      "[4 ,  1420] loss = 3.026\n",
      "[4 ,  1430] loss = 3.577\n",
      "[4 ,  1440] loss = 3.905\n",
      "[4 ,  1450] loss = 3.337\n",
      "[4 ,  1460] loss = 3.261\n",
      "[4 ,  1470] loss = 3.425\n",
      "[4 ,  1480] loss = 3.360\n",
      "[4 ,  1490] loss = 4.157\n",
      "[4 ,  1500] loss = 3.020\n",
      "[4 ,  1510] loss = 2.986\n",
      "[4 ,  1520] loss = 3.274\n",
      "[4 ,  1530] loss = 3.815\n",
      "[4 ,  1540] loss = 3.332\n",
      "[4 ,  1550] loss = 3.047\n",
      "[4 ,  1560] loss = 3.490\n",
      "[4 ,  1570] loss = 3.746\n",
      "[4 ,  1580] loss = 3.666\n",
      "[4 ,  1590] loss = 3.382\n",
      "[4 ,  1600] loss = 2.912\n",
      "[4 ,  1610] loss = 3.663\n",
      "[4 ,  1620] loss = 3.267\n",
      "[4 ,  1630] loss = 3.573\n",
      "[4 ,  1640] loss = 3.063\n",
      "[4 ,  1650] loss = 3.420\n",
      "[4 ,  1660] loss = 3.595\n",
      "[4 ,  1670] loss = 4.030\n",
      "[4 ,  1680] loss = 3.296\n",
      "[4 ,  1690] loss = 3.480\n",
      "[4 ,  1700] loss = 3.269\n",
      "[4 ,  1710] loss = 3.626\n",
      "[4 ,  1720] loss = 3.692\n",
      "[4 ,  1730] loss = 3.845\n",
      "[4 ,  1740] loss = 3.504\n",
      "[4 ,  1750] loss = 3.560\n",
      "[4 ,  1760] loss = 3.159\n",
      "[4 ,  1770] loss = 3.290\n",
      "[4 ,  1780] loss = 3.104\n",
      "[4 ,  1790] loss = 2.947\n",
      "[4 ,  1800] loss = 3.077\n",
      "[4 ,  1810] loss = 3.453\n",
      "[4 ,  1820] loss = 3.647\n",
      "[4 ,  1830] loss = 3.613\n",
      "[4 ,  1840] loss = 3.476\n",
      "[4 ,  1850] loss = 3.461\n",
      "[4 ,  1860] loss = 3.581\n",
      "[4 ,  1870] loss = 3.492\n",
      "[4 ,  1880] loss = 3.731\n",
      "[4 ,  1890] loss = 3.167\n",
      "[4 ,  1900] loss = 3.089\n",
      "[4 ,  1910] loss = 3.260\n",
      "[4 ,  1920] loss = 3.540\n",
      "[4 ,  1930] loss = 3.194\n",
      "[4 ,  1940] loss = 3.465\n",
      "[4 ,  1950] loss = 3.587\n",
      "[4 ,  1960] loss = 3.507\n",
      "[4 ,  1970] loss = 3.881\n",
      "[4 ,  1980] loss = 3.403\n",
      "[4 ,  1990] loss = 3.204\n",
      "[4 ,  2000] loss = 3.134\n",
      "[4 ,  2010] loss = 3.162\n",
      "[4 ,  2020] loss = 3.458\n",
      "[4 ,  2030] loss = 3.517\n",
      "[4 ,  2040] loss = 3.721\n",
      "[4 ,  2050] loss = 3.050\n",
      "[4 ,  2060] loss = 2.932\n",
      "[4 ,  2070] loss = 3.008\n",
      "[4 ,  2080] loss = 3.568\n",
      "[4 ,  2090] loss = 3.965\n",
      "[4 ,  2100] loss = 3.281\n",
      "[4 ,  2110] loss = 3.511\n",
      "[4 ,  2120] loss = 3.289\n",
      "[4 ,  2130] loss = 3.319\n",
      "[4 ,  2140] loss = 3.623\n",
      "[4 ,  2150] loss = 3.441\n",
      "[4 ,  2160] loss = 3.881\n",
      "[4 ,  2170] loss = 2.740\n",
      "[4 ,  2180] loss = 4.003\n",
      "[4 ,  2190] loss = 4.003\n",
      "[4 ,  2200] loss = 3.738\n",
      "[4 ,  2210] loss = 3.361\n",
      "[4 ,  2220] loss = 3.869\n",
      "[4 ,  2230] loss = 3.705\n",
      "[4 ,  2240] loss = 3.424\n",
      "[4 ,  2250] loss = 3.670\n",
      "[4 ,  2260] loss = 3.399\n",
      "[4 ,  2270] loss = 3.448\n",
      "[4 ,  2280] loss = 3.595\n",
      "[4 ,  2290] loss = 3.686\n",
      "[4 ,  2300] loss = 3.522\n",
      "[4 ,  2310] loss = 2.957\n",
      "[4 ,  2320] loss = 3.951\n",
      "[4 ,  2330] loss = 3.241\n",
      "[4 ,  2340] loss = 3.647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 ,  2350] loss = 3.330\n",
      "[4 ,  2360] loss = 3.592\n",
      "[4 ,  2370] loss = 4.151\n",
      "[4 ,  2380] loss = 3.927\n",
      "[4 ,  2390] loss = 3.319\n",
      "[4 ,  2400] loss = 3.478\n",
      "[4 ,  2410] loss = 3.466\n",
      "[4 ,  2420] loss = 3.641\n",
      "[4 ,  2430] loss = 3.919\n",
      "[4 ,  2440] loss = 3.351\n",
      "[4 ,  2450] loss = 3.447\n",
      "[4 ,  2460] loss = 4.045\n",
      "[4 ,  2470] loss = 3.841\n",
      "[4 ,  2480] loss = 3.276\n",
      "[4 ,  2490] loss = 3.762\n",
      "[4 ,  2500] loss = 3.392\n",
      "[5 ,    10] loss = 3.702\n",
      "[5 ,    20] loss = 2.754\n",
      "[5 ,    30] loss = 2.951\n",
      "[5 ,    40] loss = 2.806\n",
      "[5 ,    50] loss = 2.818\n",
      "[5 ,    60] loss = 2.718\n",
      "[5 ,    70] loss = 2.568\n",
      "[5 ,    80] loss = 3.355\n",
      "[5 ,    90] loss = 2.490\n",
      "[5 ,   100] loss = 3.158\n",
      "[5 ,   110] loss = 3.430\n",
      "[5 ,   120] loss = 3.243\n",
      "[5 ,   130] loss = 2.989\n",
      "[5 ,   140] loss = 3.097\n",
      "[5 ,   150] loss = 3.140\n",
      "[5 ,   160] loss = 3.000\n",
      "[5 ,   170] loss = 3.005\n",
      "[5 ,   180] loss = 3.120\n",
      "[5 ,   190] loss = 2.868\n",
      "[5 ,   200] loss = 2.744\n",
      "[5 ,   210] loss = 2.813\n",
      "[5 ,   220] loss = 2.975\n",
      "[5 ,   230] loss = 2.880\n",
      "[5 ,   240] loss = 3.439\n",
      "[5 ,   250] loss = 2.845\n",
      "[5 ,   260] loss = 3.540\n",
      "[5 ,   270] loss = 2.956\n",
      "[5 ,   280] loss = 3.014\n",
      "[5 ,   290] loss = 2.949\n",
      "[5 ,   300] loss = 3.056\n",
      "[5 ,   310] loss = 3.314\n",
      "[5 ,   320] loss = 3.136\n",
      "[5 ,   330] loss = 3.101\n",
      "[5 ,   340] loss = 3.044\n",
      "[5 ,   350] loss = 2.935\n",
      "[5 ,   360] loss = 2.900\n",
      "[5 ,   370] loss = 3.024\n",
      "[5 ,   380] loss = 3.122\n",
      "[5 ,   390] loss = 2.842\n",
      "[5 ,   400] loss = 3.003\n",
      "[5 ,   410] loss = 2.930\n",
      "[5 ,   420] loss = 3.355\n",
      "[5 ,   430] loss = 3.497\n",
      "[5 ,   440] loss = 2.608\n",
      "[5 ,   450] loss = 3.096\n",
      "[5 ,   460] loss = 3.082\n",
      "[5 ,   470] loss = 2.918\n",
      "[5 ,   480] loss = 3.071\n",
      "[5 ,   490] loss = 3.023\n",
      "[5 ,   500] loss = 2.971\n",
      "[5 ,   510] loss = 3.021\n",
      "[5 ,   520] loss = 3.181\n",
      "[5 ,   530] loss = 2.593\n",
      "[5 ,   540] loss = 3.278\n",
      "[5 ,   550] loss = 3.615\n",
      "[5 ,   560] loss = 2.987\n",
      "[5 ,   570] loss = 2.737\n",
      "[5 ,   580] loss = 2.662\n",
      "[5 ,   590] loss = 3.057\n",
      "[5 ,   600] loss = 3.260\n",
      "[5 ,   610] loss = 2.835\n",
      "[5 ,   620] loss = 3.173\n",
      "[5 ,   630] loss = 3.026\n",
      "[5 ,   640] loss = 3.094\n",
      "[5 ,   650] loss = 3.460\n",
      "[5 ,   660] loss = 3.394\n",
      "[5 ,   670] loss = 2.969\n",
      "[5 ,   680] loss = 3.007\n",
      "[5 ,   690] loss = 2.695\n",
      "[5 ,   700] loss = 3.031\n",
      "[5 ,   710] loss = 3.875\n",
      "[5 ,   720] loss = 3.193\n",
      "[5 ,   730] loss = 3.045\n",
      "[5 ,   740] loss = 2.908\n",
      "[5 ,   750] loss = 3.239\n",
      "[5 ,   760] loss = 3.223\n",
      "[5 ,   770] loss = 3.552\n",
      "[5 ,   780] loss = 2.722\n",
      "[5 ,   790] loss = 3.003\n",
      "[5 ,   800] loss = 2.853\n",
      "[5 ,   810] loss = 3.029\n",
      "[5 ,   820] loss = 2.910\n",
      "[5 ,   830] loss = 3.195\n",
      "[5 ,   840] loss = 2.903\n",
      "[5 ,   850] loss = 3.077\n",
      "[5 ,   860] loss = 3.099\n",
      "[5 ,   870] loss = 3.362\n",
      "[5 ,   880] loss = 2.618\n",
      "[5 ,   890] loss = 3.018\n",
      "[5 ,   900] loss = 3.044\n",
      "[5 ,   910] loss = 3.186\n",
      "[5 ,   920] loss = 4.135\n",
      "[5 ,   930] loss = 3.104\n",
      "[5 ,   940] loss = 3.023\n",
      "[5 ,   950] loss = 3.593\n",
      "[5 ,   960] loss = 3.136\n",
      "[5 ,   970] loss = 2.689\n",
      "[5 ,   980] loss = 2.750\n",
      "[5 ,   990] loss = 2.900\n",
      "[5 ,  1000] loss = 2.931\n",
      "[5 ,  1010] loss = 2.968\n",
      "[5 ,  1020] loss = 3.073\n",
      "[5 ,  1030] loss = 3.291\n",
      "[5 ,  1040] loss = 2.874\n",
      "[5 ,  1050] loss = 3.187\n",
      "[5 ,  1060] loss = 3.051\n",
      "[5 ,  1070] loss = 3.225\n",
      "[5 ,  1080] loss = 3.026\n",
      "[5 ,  1090] loss = 3.251\n",
      "[5 ,  1100] loss = 3.091\n",
      "[5 ,  1110] loss = 3.307\n",
      "[5 ,  1120] loss = 3.075\n",
      "[5 ,  1130] loss = 2.957\n",
      "[5 ,  1140] loss = 3.139\n",
      "[5 ,  1150] loss = 2.986\n",
      "[5 ,  1160] loss = 3.110\n",
      "[5 ,  1170] loss = 2.807\n",
      "[5 ,  1180] loss = 3.380\n",
      "[5 ,  1190] loss = 3.023\n",
      "[5 ,  1200] loss = 3.261\n",
      "[5 ,  1210] loss = 3.392\n",
      "[5 ,  1220] loss = 3.514\n",
      "[5 ,  1230] loss = 3.444\n",
      "[5 ,  1240] loss = 3.103\n",
      "[5 ,  1250] loss = 3.331\n",
      "[5 ,  1260] loss = 2.946\n",
      "[5 ,  1270] loss = 3.008\n",
      "[5 ,  1280] loss = 3.105\n",
      "[5 ,  1290] loss = 3.148\n",
      "[5 ,  1300] loss = 3.175\n",
      "[5 ,  1310] loss = 3.791\n",
      "[5 ,  1320] loss = 3.442\n",
      "[5 ,  1330] loss = 3.218\n",
      "[5 ,  1340] loss = 2.852\n",
      "[5 ,  1350] loss = 2.610\n",
      "[5 ,  1360] loss = 3.198\n",
      "[5 ,  1370] loss = 2.929\n",
      "[5 ,  1380] loss = 3.016\n",
      "[5 ,  1390] loss = 2.947\n",
      "[5 ,  1400] loss = 3.406\n",
      "[5 ,  1410] loss = 2.774\n",
      "[5 ,  1420] loss = 3.546\n",
      "[5 ,  1430] loss = 2.928\n",
      "[5 ,  1440] loss = 3.263\n",
      "[5 ,  1450] loss = 2.981\n",
      "[5 ,  1460] loss = 2.735\n",
      "[5 ,  1470] loss = 2.893\n",
      "[5 ,  1480] loss = 3.241\n",
      "[5 ,  1490] loss = 3.355\n",
      "[5 ,  1500] loss = 3.401\n",
      "[5 ,  1510] loss = 3.589\n",
      "[5 ,  1520] loss = 3.939\n",
      "[5 ,  1530] loss = 3.507\n",
      "[5 ,  1540] loss = 3.310\n",
      "[5 ,  1550] loss = 3.227\n",
      "[5 ,  1560] loss = 3.492\n",
      "[5 ,  1570] loss = 3.083\n",
      "[5 ,  1580] loss = 3.180\n",
      "[5 ,  1590] loss = 3.085\n",
      "[5 ,  1600] loss = 2.784\n",
      "[5 ,  1610] loss = 3.001\n",
      "[5 ,  1620] loss = 3.230\n",
      "[5 ,  1630] loss = 3.184\n",
      "[5 ,  1640] loss = 3.309\n",
      "[5 ,  1650] loss = 3.348\n",
      "[5 ,  1660] loss = 3.637\n",
      "[5 ,  1670] loss = 3.156\n",
      "[5 ,  1680] loss = 3.256\n",
      "[5 ,  1690] loss = 3.106\n",
      "[5 ,  1700] loss = 2.963\n",
      "[5 ,  1710] loss = 3.180\n",
      "[5 ,  1720] loss = 3.104\n",
      "[5 ,  1730] loss = 2.983\n",
      "[5 ,  1740] loss = 3.301\n",
      "[5 ,  1750] loss = 3.056\n",
      "[5 ,  1760] loss = 4.125\n",
      "[5 ,  1770] loss = 3.168\n",
      "[5 ,  1780] loss = 3.025\n",
      "[5 ,  1790] loss = 3.247\n",
      "[5 ,  1800] loss = 2.874\n",
      "[5 ,  1810] loss = 3.063\n",
      "[5 ,  1820] loss = 2.987\n",
      "[5 ,  1830] loss = 3.316\n",
      "[5 ,  1840] loss = 2.998\n",
      "[5 ,  1850] loss = 3.169\n",
      "[5 ,  1860] loss = 3.549\n",
      "[5 ,  1870] loss = 3.146\n",
      "[5 ,  1880] loss = 3.018\n",
      "[5 ,  1890] loss = 2.975\n",
      "[5 ,  1900] loss = 3.053\n",
      "[5 ,  1910] loss = 3.320\n",
      "[5 ,  1920] loss = 3.265\n",
      "[5 ,  1930] loss = 3.612\n",
      "[5 ,  1940] loss = 3.224\n",
      "[5 ,  1950] loss = 3.180\n",
      "[5 ,  1960] loss = 3.115\n",
      "[5 ,  1970] loss = 3.371\n",
      "[5 ,  1980] loss = 2.899\n",
      "[5 ,  1990] loss = 3.456\n",
      "[5 ,  2000] loss = 3.087\n",
      "[5 ,  2010] loss = 3.143\n",
      "[5 ,  2020] loss = 3.166\n",
      "[5 ,  2030] loss = 2.963\n",
      "[5 ,  2040] loss = 3.032\n",
      "[5 ,  2050] loss = 3.206\n",
      "[5 ,  2060] loss = 3.633\n",
      "[5 ,  2070] loss = 2.957\n",
      "[5 ,  2080] loss = 2.976\n",
      "[5 ,  2090] loss = 3.592\n",
      "[5 ,  2100] loss = 3.417\n",
      "[5 ,  2110] loss = 3.003\n",
      "[5 ,  2120] loss = 2.984\n",
      "[5 ,  2130] loss = 3.427\n",
      "[5 ,  2140] loss = 3.310\n",
      "[5 ,  2150] loss = 3.655\n",
      "[5 ,  2160] loss = 3.756\n",
      "[5 ,  2170] loss = 3.071\n",
      "[5 ,  2180] loss = 3.075\n",
      "[5 ,  2190] loss = 3.257\n",
      "[5 ,  2200] loss = 3.016\n",
      "[5 ,  2210] loss = 3.200\n",
      "[5 ,  2220] loss = 3.552\n",
      "[5 ,  2230] loss = 3.241\n",
      "[5 ,  2240] loss = 2.967\n",
      "[5 ,  2250] loss = 3.096\n",
      "[5 ,  2260] loss = 3.318\n",
      "[5 ,  2270] loss = 3.555\n",
      "[5 ,  2280] loss = 3.434\n",
      "[5 ,  2290] loss = 2.996\n",
      "[5 ,  2300] loss = 2.794\n",
      "[5 ,  2310] loss = 3.427\n",
      "[5 ,  2320] loss = 3.059\n",
      "[5 ,  2330] loss = 2.933\n",
      "[5 ,  2340] loss = 3.383\n",
      "[5 ,  2350] loss = 3.193\n",
      "[5 ,  2360] loss = 3.142\n",
      "[5 ,  2370] loss = 2.937\n",
      "[5 ,  2380] loss = 2.817\n",
      "[5 ,  2390] loss = 3.594\n",
      "[5 ,  2400] loss = 3.631\n",
      "[5 ,  2410] loss = 3.138\n",
      "[5 ,  2420] loss = 3.187\n",
      "[5 ,  2430] loss = 2.950\n",
      "[5 ,  2440] loss = 3.077\n",
      "[5 ,  2450] loss = 3.144\n",
      "[5 ,  2460] loss = 3.449\n",
      "[5 ,  2470] loss = 3.507\n",
      "[5 ,  2480] loss = 3.571\n",
      "[5 ,  2490] loss = 3.058\n",
      "[5 ,  2500] loss = 3.167\n",
      "[6 ,    10] loss = 2.722\n",
      "[6 ,    20] loss = 2.622\n",
      "[6 ,    30] loss = 3.327\n",
      "[6 ,    40] loss = 2.617\n",
      "[6 ,    50] loss = 2.603\n",
      "[6 ,    60] loss = 2.525\n",
      "[6 ,    70] loss = 2.658\n",
      "[6 ,    80] loss = 2.522\n",
      "[6 ,    90] loss = 2.622\n",
      "[6 ,   100] loss = 2.647\n",
      "[6 ,   110] loss = 2.565\n",
      "[6 ,   120] loss = 2.711\n",
      "[6 ,   130] loss = 2.722\n",
      "[6 ,   140] loss = 2.718\n",
      "[6 ,   150] loss = 2.424\n",
      "[6 ,   160] loss = 2.492\n",
      "[6 ,   170] loss = 2.637\n",
      "[6 ,   180] loss = 2.664\n",
      "[6 ,   190] loss = 2.822\n",
      "[6 ,   200] loss = 2.792\n",
      "[6 ,   210] loss = 3.161\n",
      "[6 ,   220] loss = 2.614\n",
      "[6 ,   230] loss = 2.447\n",
      "[6 ,   240] loss = 2.569\n",
      "[6 ,   250] loss = 2.611\n",
      "[6 ,   260] loss = 2.697\n",
      "[6 ,   270] loss = 2.561\n",
      "[6 ,   280] loss = 2.954\n",
      "[6 ,   290] loss = 2.511\n",
      "[6 ,   300] loss = 2.832\n",
      "[6 ,   310] loss = 2.705\n",
      "[6 ,   320] loss = 2.969\n",
      "[6 ,   330] loss = 2.571\n",
      "[6 ,   340] loss = 2.623\n",
      "[6 ,   350] loss = 2.535\n",
      "[6 ,   360] loss = 2.874\n",
      "[6 ,   370] loss = 2.716\n",
      "[6 ,   380] loss = 2.683\n",
      "[6 ,   390] loss = 3.162\n",
      "[6 ,   400] loss = 2.721\n",
      "[6 ,   410] loss = 2.722\n",
      "[6 ,   420] loss = 2.606\n",
      "[6 ,   430] loss = 2.772\n",
      "[6 ,   440] loss = 2.692\n",
      "[6 ,   450] loss = 2.762\n",
      "[6 ,   460] loss = 2.835\n",
      "[6 ,   470] loss = 3.239\n",
      "[6 ,   480] loss = 2.831\n",
      "[6 ,   490] loss = 2.424\n",
      "[6 ,   500] loss = 2.744\n",
      "[6 ,   510] loss = 2.954\n",
      "[6 ,   520] loss = 2.624\n",
      "[6 ,   530] loss = 2.461\n",
      "[6 ,   540] loss = 2.722\n",
      "[6 ,   550] loss = 2.812\n",
      "[6 ,   560] loss = 2.579\n",
      "[6 ,   570] loss = 2.698\n",
      "[6 ,   580] loss = 2.800\n",
      "[6 ,   590] loss = 2.722\n",
      "[6 ,   600] loss = 2.811\n",
      "[6 ,   610] loss = 2.842\n",
      "[6 ,   620] loss = 2.910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 ,   630] loss = 2.823\n",
      "[6 ,   640] loss = 2.768\n",
      "[6 ,   650] loss = 2.581\n",
      "[6 ,   660] loss = 2.800\n",
      "[6 ,   670] loss = 2.820\n",
      "[6 ,   680] loss = 2.619\n",
      "[6 ,   690] loss = 2.626\n",
      "[6 ,   700] loss = 2.830\n",
      "[6 ,   710] loss = 2.809\n",
      "[6 ,   720] loss = 2.788\n",
      "[6 ,   730] loss = 2.695\n",
      "[6 ,   740] loss = 2.893\n",
      "[6 ,   750] loss = 2.802\n",
      "[6 ,   760] loss = 3.193\n",
      "[6 ,   770] loss = 2.977\n",
      "[6 ,   780] loss = 2.821\n",
      "[6 ,   790] loss = 2.501\n",
      "[6 ,   800] loss = 2.915\n",
      "[6 ,   810] loss = 3.202\n",
      "[6 ,   820] loss = 3.051\n",
      "[6 ,   830] loss = 3.098\n",
      "[6 ,   840] loss = 2.941\n",
      "[6 ,   850] loss = 2.687\n",
      "[6 ,   860] loss = 3.208\n",
      "[6 ,   870] loss = 2.912\n",
      "[6 ,   880] loss = 3.111\n",
      "[6 ,   890] loss = 2.932\n",
      "[6 ,   900] loss = 2.953\n",
      "[6 ,   910] loss = 2.908\n",
      "[6 ,   920] loss = 2.888\n",
      "[6 ,   930] loss = 3.245\n",
      "[6 ,   940] loss = 2.584\n",
      "[6 ,   950] loss = 2.581\n",
      "[6 ,   960] loss = 2.973\n",
      "[6 ,   970] loss = 2.717\n",
      "[6 ,   980] loss = 3.074\n",
      "[6 ,   990] loss = 2.692\n",
      "[6 ,  1000] loss = 2.735\n",
      "[6 ,  1010] loss = 2.786\n",
      "[6 ,  1020] loss = 2.763\n",
      "[6 ,  1030] loss = 3.397\n",
      "[6 ,  1040] loss = 2.661\n",
      "[6 ,  1050] loss = 2.677\n",
      "[6 ,  1060] loss = 3.279\n",
      "[6 ,  1070] loss = 2.712\n",
      "[6 ,  1080] loss = 2.965\n",
      "[6 ,  1090] loss = 2.833\n",
      "[6 ,  1100] loss = 2.567\n",
      "[6 ,  1110] loss = 3.340\n",
      "[6 ,  1120] loss = 2.755\n",
      "[6 ,  1130] loss = 2.925\n",
      "[6 ,  1140] loss = 2.609\n",
      "[6 ,  1150] loss = 2.995\n",
      "[6 ,  1160] loss = 2.962\n",
      "[6 ,  1170] loss = 2.839\n",
      "[6 ,  1180] loss = 2.630\n",
      "[6 ,  1190] loss = 2.886\n",
      "[6 ,  1200] loss = 2.625\n",
      "[6 ,  1210] loss = 3.020\n",
      "[6 ,  1220] loss = 3.102\n",
      "[6 ,  1230] loss = 2.917\n",
      "[6 ,  1240] loss = 3.107\n",
      "[6 ,  1250] loss = 2.988\n",
      "[6 ,  1260] loss = 2.844\n",
      "[6 ,  1270] loss = 3.170\n",
      "[6 ,  1280] loss = 2.978\n",
      "[6 ,  1290] loss = 2.776\n",
      "[6 ,  1300] loss = 2.759\n",
      "[6 ,  1310] loss = 3.089\n",
      "[6 ,  1320] loss = 2.988\n",
      "[6 ,  1330] loss = 2.713\n",
      "[6 ,  1340] loss = 2.780\n",
      "[6 ,  1350] loss = 2.876\n",
      "[6 ,  1360] loss = 2.781\n",
      "[6 ,  1370] loss = 2.806\n",
      "[6 ,  1380] loss = 2.876\n",
      "[6 ,  1390] loss = 2.966\n",
      "[6 ,  1400] loss = 2.896\n",
      "[6 ,  1410] loss = 2.839\n",
      "[6 ,  1420] loss = 2.613\n",
      "[6 ,  1430] loss = 2.498\n",
      "[6 ,  1440] loss = 2.733\n",
      "[6 ,  1450] loss = 3.032\n",
      "[6 ,  1460] loss = 2.539\n",
      "[6 ,  1470] loss = 2.667\n",
      "[6 ,  1480] loss = 3.214\n",
      "[6 ,  1490] loss = 3.064\n",
      "[6 ,  1500] loss = 2.705\n",
      "[6 ,  1510] loss = 2.984\n",
      "[6 ,  1520] loss = 2.766\n",
      "[6 ,  1530] loss = 2.982\n",
      "[6 ,  1540] loss = 2.752\n",
      "[6 ,  1550] loss = 2.900\n",
      "[6 ,  1560] loss = 2.723\n",
      "[6 ,  1570] loss = 2.579\n",
      "[6 ,  1580] loss = 2.762\n",
      "[6 ,  1590] loss = 2.687\n",
      "[6 ,  1600] loss = 2.511\n",
      "[6 ,  1610] loss = 2.961\n",
      "[6 ,  1620] loss = 2.869\n",
      "[6 ,  1630] loss = 2.848\n",
      "[6 ,  1640] loss = 2.674\n",
      "[6 ,  1650] loss = 2.912\n",
      "[6 ,  1660] loss = 3.566\n",
      "[6 ,  1670] loss = 2.955\n",
      "[6 ,  1680] loss = 2.684\n",
      "[6 ,  1690] loss = 2.822\n",
      "[6 ,  1700] loss = 3.199\n",
      "[6 ,  1710] loss = 2.944\n",
      "[6 ,  1720] loss = 3.288\n",
      "[6 ,  1730] loss = 2.745\n",
      "[6 ,  1740] loss = 2.698\n",
      "[6 ,  1750] loss = 3.188\n",
      "[6 ,  1760] loss = 3.157\n",
      "[6 ,  1770] loss = 2.954\n",
      "[6 ,  1780] loss = 2.665\n",
      "[6 ,  1790] loss = 2.865\n",
      "[6 ,  1800] loss = 2.885\n",
      "[6 ,  1810] loss = 2.723\n",
      "[6 ,  1820] loss = 3.743\n",
      "[6 ,  1830] loss = 2.797\n",
      "[6 ,  1840] loss = 2.435\n",
      "[6 ,  1850] loss = 2.577\n",
      "[6 ,  1860] loss = 2.709\n",
      "[6 ,  1870] loss = 2.672\n",
      "[6 ,  1880] loss = 2.809\n",
      "[6 ,  1890] loss = 2.643\n",
      "[6 ,  1900] loss = 2.886\n",
      "[6 ,  1910] loss = 2.785\n",
      "[6 ,  1920] loss = 2.983\n",
      "[6 ,  1930] loss = 3.165\n",
      "[6 ,  1940] loss = 2.955\n",
      "[6 ,  1950] loss = 3.479\n",
      "[6 ,  1960] loss = 3.041\n",
      "[6 ,  1970] loss = 3.413\n",
      "[6 ,  1980] loss = 3.052\n",
      "[6 ,  1990] loss = 3.187\n",
      "[6 ,  2000] loss = 2.983\n",
      "[6 ,  2010] loss = 3.012\n",
      "[6 ,  2020] loss = 2.866\n",
      "[6 ,  2030] loss = 2.664\n",
      "[6 ,  2040] loss = 2.733\n",
      "[6 ,  2050] loss = 2.707\n",
      "[6 ,  2060] loss = 3.039\n",
      "[6 ,  2070] loss = 2.685\n",
      "[6 ,  2080] loss = 3.108\n",
      "[6 ,  2090] loss = 3.153\n",
      "[6 ,  2100] loss = 3.241\n",
      "[6 ,  2110] loss = 3.153\n",
      "[6 ,  2120] loss = 3.023\n",
      "[6 ,  2130] loss = 3.288\n",
      "[6 ,  2140] loss = 2.676\n",
      "[6 ,  2150] loss = 2.932\n",
      "[6 ,  2160] loss = 2.600\n",
      "[6 ,  2170] loss = 3.118\n",
      "[6 ,  2180] loss = 3.214\n",
      "[6 ,  2190] loss = 2.634\n",
      "[6 ,  2200] loss = 2.696\n",
      "[6 ,  2210] loss = 2.744\n",
      "[6 ,  2220] loss = 3.424\n",
      "[6 ,  2230] loss = 3.078\n",
      "[6 ,  2240] loss = 2.948\n",
      "[6 ,  2250] loss = 3.468\n",
      "[6 ,  2260] loss = 2.856\n",
      "[6 ,  2270] loss = 2.963\n",
      "[6 ,  2280] loss = 3.496\n",
      "[6 ,  2290] loss = 3.134\n",
      "[6 ,  2300] loss = 3.526\n",
      "[6 ,  2310] loss = 2.925\n",
      "[6 ,  2320] loss = 3.153\n",
      "[6 ,  2330] loss = 3.304\n",
      "[6 ,  2340] loss = 3.114\n",
      "[6 ,  2350] loss = 3.213\n",
      "[6 ,  2360] loss = 2.440\n",
      "[6 ,  2370] loss = 2.943\n",
      "[6 ,  2380] loss = 2.896\n",
      "[6 ,  2390] loss = 2.962\n",
      "[6 ,  2400] loss = 2.962\n",
      "[6 ,  2410] loss = 2.813\n",
      "[6 ,  2420] loss = 3.120\n",
      "[6 ,  2430] loss = 2.872\n",
      "[6 ,  2440] loss = 2.666\n",
      "[6 ,  2450] loss = 2.794\n",
      "[6 ,  2460] loss = 3.630\n",
      "[6 ,  2470] loss = 2.930\n",
      "[6 ,  2480] loss = 2.689\n",
      "[6 ,  2490] loss = 2.674\n",
      "[6 ,  2500] loss = 2.687\n",
      "[7 ,    10] loss = 2.470\n",
      "[7 ,    20] loss = 2.463\n",
      "[7 ,    30] loss = 2.633\n",
      "[7 ,    40] loss = 2.624\n",
      "[7 ,    50] loss = 2.143\n",
      "[7 ,    60] loss = 2.326\n",
      "[7 ,    70] loss = 2.346\n",
      "[7 ,    80] loss = 2.519\n",
      "[7 ,    90] loss = 2.276\n",
      "[7 ,   100] loss = 2.247\n",
      "[7 ,   110] loss = 2.534\n",
      "[7 ,   120] loss = 2.307\n",
      "[7 ,   130] loss = 2.237\n",
      "[7 ,   140] loss = 2.512\n",
      "[7 ,   150] loss = 2.256\n",
      "[7 ,   160] loss = 2.352\n",
      "[7 ,   170] loss = 2.204\n",
      "[7 ,   180] loss = 2.427\n",
      "[7 ,   190] loss = 2.576\n",
      "[7 ,   200] loss = 2.287\n",
      "[7 ,   210] loss = 2.348\n",
      "[7 ,   220] loss = 2.394\n",
      "[7 ,   230] loss = 2.499\n",
      "[7 ,   240] loss = 2.194\n",
      "[7 ,   250] loss = 2.265\n",
      "[7 ,   260] loss = 2.538\n",
      "[7 ,   270] loss = 2.567\n",
      "[7 ,   280] loss = 2.549\n",
      "[7 ,   290] loss = 2.357\n",
      "[7 ,   300] loss = 2.523\n",
      "[7 ,   310] loss = 2.524\n",
      "[7 ,   320] loss = 2.250\n",
      "[7 ,   330] loss = 2.741\n",
      "[7 ,   340] loss = 2.725\n",
      "[7 ,   350] loss = 2.387\n",
      "[7 ,   360] loss = 2.522\n",
      "[7 ,   370] loss = 2.488\n",
      "[7 ,   380] loss = 2.628\n",
      "[7 ,   390] loss = 2.685\n",
      "[7 ,   400] loss = 2.313\n",
      "[7 ,   410] loss = 2.522\n",
      "[7 ,   420] loss = 2.257\n",
      "[7 ,   430] loss = 2.715\n",
      "[7 ,   440] loss = 2.603\n",
      "[7 ,   450] loss = 2.673\n",
      "[7 ,   460] loss = 2.407\n",
      "[7 ,   470] loss = 2.519\n",
      "[7 ,   480] loss = 2.499\n",
      "[7 ,   490] loss = 2.341\n",
      "[7 ,   500] loss = 2.335\n",
      "[7 ,   510] loss = 2.586\n",
      "[7 ,   520] loss = 2.541\n",
      "[7 ,   530] loss = 2.314\n",
      "[7 ,   540] loss = 2.388\n",
      "[7 ,   550] loss = 2.491\n",
      "[7 ,   560] loss = 2.433\n",
      "[7 ,   570] loss = 2.568\n",
      "[7 ,   580] loss = 2.898\n",
      "[7 ,   590] loss = 2.462\n",
      "[7 ,   600] loss = 2.460\n",
      "[7 ,   610] loss = 2.616\n",
      "[7 ,   620] loss = 2.301\n",
      "[7 ,   630] loss = 2.521\n",
      "[7 ,   640] loss = 2.285\n",
      "[7 ,   650] loss = 2.446\n",
      "[7 ,   660] loss = 2.446\n",
      "[7 ,   670] loss = 2.189\n",
      "[7 ,   680] loss = 2.516\n",
      "[7 ,   690] loss = 2.587\n",
      "[7 ,   700] loss = 2.590\n",
      "[7 ,   710] loss = 2.570\n",
      "[7 ,   720] loss = 2.411\n",
      "[7 ,   730] loss = 2.248\n",
      "[7 ,   740] loss = 2.364\n",
      "[7 ,   750] loss = 2.383\n",
      "[7 ,   760] loss = 2.622\n",
      "[7 ,   770] loss = 2.386\n",
      "[7 ,   780] loss = 2.465\n",
      "[7 ,   790] loss = 2.742\n",
      "[7 ,   800] loss = 2.658\n",
      "[7 ,   810] loss = 2.735\n",
      "[7 ,   820] loss = 2.641\n",
      "[7 ,   830] loss = 2.653\n",
      "[7 ,   840] loss = 2.680\n",
      "[7 ,   850] loss = 2.806\n",
      "[7 ,   860] loss = 2.395\n",
      "[7 ,   870] loss = 2.547\n",
      "[7 ,   880] loss = 2.815\n",
      "[7 ,   890] loss = 2.510\n",
      "[7 ,   900] loss = 2.529\n",
      "[7 ,   910] loss = 2.445\n",
      "[7 ,   920] loss = 2.575\n",
      "[7 ,   930] loss = 2.630\n",
      "[7 ,   940] loss = 2.540\n",
      "[7 ,   950] loss = 2.315\n",
      "[7 ,   960] loss = 2.731\n",
      "[7 ,   970] loss = 2.262\n",
      "[7 ,   980] loss = 2.728\n",
      "[7 ,   990] loss = 2.264\n",
      "[7 ,  1000] loss = 2.361\n",
      "[7 ,  1010] loss = 2.246\n",
      "[7 ,  1020] loss = 2.530\n",
      "[7 ,  1030] loss = 2.969\n",
      "[7 ,  1040] loss = 2.453\n",
      "[7 ,  1050] loss = 2.482\n",
      "[7 ,  1060] loss = 3.136\n",
      "[7 ,  1070] loss = 2.567\n",
      "[7 ,  1080] loss = 2.772\n",
      "[7 ,  1090] loss = 2.704\n",
      "[7 ,  1100] loss = 2.592\n",
      "[7 ,  1110] loss = 2.817\n",
      "[7 ,  1120] loss = 2.365\n",
      "[7 ,  1130] loss = 2.764\n",
      "[7 ,  1140] loss = 2.382\n",
      "[7 ,  1150] loss = 2.407\n",
      "[7 ,  1160] loss = 2.312\n",
      "[7 ,  1170] loss = 2.500\n",
      "[7 ,  1180] loss = 2.402\n",
      "[7 ,  1190] loss = 2.437\n",
      "[7 ,  1200] loss = 2.647\n",
      "[7 ,  1210] loss = 2.590\n",
      "[7 ,  1220] loss = 2.295\n",
      "[7 ,  1230] loss = 2.465\n",
      "[7 ,  1240] loss = 2.528\n",
      "[7 ,  1250] loss = 2.694\n",
      "[7 ,  1260] loss = 2.542\n",
      "[7 ,  1270] loss = 2.722\n",
      "[7 ,  1280] loss = 2.924\n",
      "[7 ,  1290] loss = 2.749\n",
      "[7 ,  1300] loss = 2.835\n",
      "[7 ,  1310] loss = 2.466\n",
      "[7 ,  1320] loss = 3.249\n",
      "[7 ,  1330] loss = 3.148\n",
      "[7 ,  1340] loss = 2.427\n",
      "[7 ,  1350] loss = 2.657\n",
      "[7 ,  1360] loss = 2.708\n",
      "[7 ,  1370] loss = 2.620\n",
      "[7 ,  1380] loss = 2.616\n",
      "[7 ,  1390] loss = 2.601\n",
      "[7 ,  1400] loss = 2.752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 ,  1410] loss = 3.077\n",
      "[7 ,  1420] loss = 2.420\n",
      "[7 ,  1430] loss = 2.637\n",
      "[7 ,  1440] loss = 2.582\n",
      "[7 ,  1450] loss = 2.544\n",
      "[7 ,  1460] loss = 2.414\n",
      "[7 ,  1470] loss = 3.045\n",
      "[7 ,  1480] loss = 2.886\n",
      "[7 ,  1490] loss = 2.730\n",
      "[7 ,  1500] loss = 2.771\n",
      "[7 ,  1510] loss = 2.259\n",
      "[7 ,  1520] loss = 2.634\n",
      "[7 ,  1530] loss = 2.587\n",
      "[7 ,  1540] loss = 2.724\n",
      "[7 ,  1550] loss = 2.580\n",
      "[7 ,  1560] loss = 2.851\n",
      "[7 ,  1570] loss = 2.730\n",
      "[7 ,  1580] loss = 2.581\n",
      "[7 ,  1590] loss = 2.490\n",
      "[7 ,  1600] loss = 2.590\n",
      "[7 ,  1610] loss = 2.451\n",
      "[7 ,  1620] loss = 2.258\n",
      "[7 ,  1630] loss = 2.407\n",
      "[7 ,  1640] loss = 2.639\n",
      "[7 ,  1650] loss = 2.530\n",
      "[7 ,  1660] loss = 2.437\n",
      "[7 ,  1670] loss = 2.528\n",
      "[7 ,  1680] loss = 2.657\n",
      "[7 ,  1690] loss = 2.747\n",
      "[7 ,  1700] loss = 2.973\n",
      "[7 ,  1710] loss = 2.404\n",
      "[7 ,  1720] loss = 2.517\n",
      "[7 ,  1730] loss = 2.632\n",
      "[7 ,  1740] loss = 2.623\n",
      "[7 ,  1750] loss = 2.796\n",
      "[7 ,  1760] loss = 2.390\n",
      "[7 ,  1770] loss = 2.767\n",
      "[7 ,  1780] loss = 2.707\n",
      "[7 ,  1790] loss = 2.303\n",
      "[7 ,  1800] loss = 2.597\n",
      "[7 ,  1810] loss = 2.545\n",
      "[7 ,  1820] loss = 2.536\n",
      "[7 ,  1830] loss = 2.645\n",
      "[7 ,  1840] loss = 2.581\n",
      "[7 ,  1850] loss = 2.513\n",
      "[7 ,  1860] loss = 2.782\n",
      "[7 ,  1870] loss = 2.568\n",
      "[7 ,  1880] loss = 3.075\n",
      "[7 ,  1890] loss = 2.760\n",
      "[7 ,  1900] loss = 2.279\n",
      "[7 ,  1910] loss = 2.472\n",
      "[7 ,  1920] loss = 2.414\n",
      "[7 ,  1930] loss = 2.540\n",
      "[7 ,  1940] loss = 2.793\n",
      "[7 ,  1950] loss = 2.477\n",
      "[7 ,  1960] loss = 2.459\n",
      "[7 ,  1970] loss = 2.955\n",
      "[7 ,  1980] loss = 2.784\n",
      "[7 ,  1990] loss = 2.491\n",
      "[7 ,  2000] loss = 2.761\n",
      "[7 ,  2010] loss = 2.828\n",
      "[7 ,  2020] loss = 2.504\n",
      "[7 ,  2030] loss = 2.825\n",
      "[7 ,  2040] loss = 2.396\n",
      "[7 ,  2050] loss = 2.452\n",
      "[7 ,  2060] loss = 2.443\n",
      "[7 ,  2070] loss = 2.584\n",
      "[7 ,  2080] loss = 2.645\n",
      "[7 ,  2090] loss = 2.850\n",
      "[7 ,  2100] loss = 2.656\n",
      "[7 ,  2110] loss = 2.983\n",
      "[7 ,  2120] loss = 2.415\n",
      "[7 ,  2130] loss = 2.882\n",
      "[7 ,  2140] loss = 2.621\n",
      "[7 ,  2150] loss = 2.607\n",
      "[7 ,  2160] loss = 2.838\n",
      "[7 ,  2170] loss = 2.586\n",
      "[7 ,  2180] loss = 2.623\n",
      "[7 ,  2190] loss = 2.417\n",
      "[7 ,  2200] loss = 2.725\n",
      "[7 ,  2210] loss = 2.354\n",
      "[7 ,  2220] loss = 2.949\n",
      "[7 ,  2230] loss = 2.411\n",
      "[7 ,  2240] loss = 2.516\n",
      "[7 ,  2250] loss = 3.055\n",
      "[7 ,  2260] loss = 2.482\n",
      "[7 ,  2270] loss = 2.950\n",
      "[7 ,  2280] loss = 2.998\n",
      "[7 ,  2290] loss = 2.648\n",
      "[7 ,  2300] loss = 2.510\n",
      "[7 ,  2310] loss = 2.580\n",
      "[7 ,  2320] loss = 2.351\n",
      "[7 ,  2330] loss = 3.116\n",
      "[7 ,  2340] loss = 2.792\n",
      "[7 ,  2350] loss = 2.492\n",
      "[7 ,  2360] loss = 2.533\n",
      "[7 ,  2370] loss = 3.113\n",
      "[7 ,  2380] loss = 2.805\n",
      "[7 ,  2390] loss = 2.430\n",
      "[7 ,  2400] loss = 2.467\n",
      "[7 ,  2410] loss = 2.716\n",
      "[7 ,  2420] loss = 2.633\n",
      "[7 ,  2430] loss = 2.684\n",
      "[7 ,  2440] loss = 2.700\n",
      "[7 ,  2450] loss = 3.401\n",
      "[7 ,  2460] loss = 2.516\n",
      "[7 ,  2470] loss = 2.730\n",
      "[7 ,  2480] loss = 2.711\n",
      "[7 ,  2490] loss = 2.353\n",
      "[7 ,  2500] loss = 2.509\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss =0\n",
    "    force_prob = (10 - epoch)*0.1 if epoch<10 else 0\n",
    "   \n",
    "    for i , data in enumerate(train_iter_tuple):\n",
    "        summary , story = data\n",
    "        optimizer.zero_grad()\n",
    "        prob = model(story , summary ,beam_width, force_prob)\n",
    "        loss = seq2seqloss(prob[0] , summary)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss +=loss.item()\n",
    "        if i%10 == 9:  #print every 10th minibatch\n",
    "            print('[%d , %5d] loss = %.3f' %(epoch+1 , i+1 , running_loss/10))\n",
    "            running_loss =0\n",
    "            \n",
    "print('Finished Training')\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_string = TEXT.vocab.itos\n",
    "string_to_index = TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict() , 'SavedTxtSumModel.pth' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (pretrained_vectors): Embedding(41230, 200, padding_idx=1)\n",
       "  (encoder_dropout): Dropout(p=0, inplace=False)\n",
       "  (encoder_embedding_layer): Embedding(41230, 200, padding_idx=1)\n",
       "  (encoder_LSTM): LSTM(200, 200, num_layers=2, bidirectional=True)\n",
       "  (encoder_vector_layer): Linear(in_features=400, out_features=200, bias=False)\n",
       "  (decoder_dropout): Dropout(p=0, inplace=False)\n",
       "  (decoder_embedding_layer): Embedding(41230, 200, padding_idx=1)\n",
       "  (decoder_LSTM): LSTM(200, 400)\n",
       "  (decoder_output_layer): Linear(in_features=400, out_features=200, bias=False)\n",
       "  (output_layer): Linear(in_features=200, out_features=41230, bias=False)\n",
       "  (encoder_output_layer): Linear(in_features=400, out_features=200, bias=False)\n",
       "  (attention_vector_layer): Linear(in_features=400, out_features=200, bias=False)\n",
       "  (decoder_hidden_layer): Linear(in_features=400, out_features=200, bias=False)\n",
       "  (attn_score): Linear(in_features=200, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to get back model parameters\n",
    "model.load_state_dict(torch.load('SavedTxtSumModel.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                           max_n=3,\n",
    "                           limit_length=True,\n",
    "                           length_limit=100,\n",
    "                           length_limit_type='words',\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE SCORE ON VALIDATION SET\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge-2': {'f_score': 0.05756711733100212,\n",
       "  'p_score': 0.06079366189366196,\n",
       "  'r_score': 0.0571871434121435},\n",
       " 'rouge-3': {'f_score': 0.01799197322285557,\n",
       "  'p_score': 0.01909047619047619,\n",
       "  'r_score': 0.017923917748917745},\n",
       " 'rouge-1': {'f_score': 0.31273702030367223,\n",
       "  'p_score': 0.3335660506160498,\n",
       "  'r_score': 0.0571871434121435},\n",
       " 'rouge-l': {'f_score': 0.3640601423673266,\n",
       "  'p_score': 0.38275283636454716,\n",
       "  'r_score': 0.35888625357357584},\n",
       " 'rouge-w': {'f_score': 0.21103159077033953,\n",
       "  'p_score': 0.2873979701562313,\n",
       "  'r_score': 0.17396290005154413}}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate results in validation dataset\n",
    "no_of_example = len(vld)\n",
    "SCORE = {'rouge-2': {'f_score': 0.0, 'p_score': 0.0, 'r_score': 0.0},      #initialize rouge score of validation \n",
    "         'rouge-3': {'f_score': 0.0, 'p_score': 0.0, 'r_score': 0.0},      #set with 0\n",
    "         'rouge-1': {'f_score': 0.0, 'p_score': 0.0, 'r_score': 0.0},\n",
    "         'rouge-l': {'f_score': 0.0, 'p_score': 0.0, 'r_score': 0.0},\n",
    "         'rouge-w': {'f_score': 0.0, 'p_score': 0.0, 'r_score': 0.0}}\n",
    "\n",
    "\n",
    "for i , data  in enumerate(valid_iter_tuple):\n",
    "    summary , article = data\n",
    "    out = model(article , None , beam_width)     #out is list \n",
    "    for k in range(len(out)):\n",
    "        \n",
    "        preds = out[k]\n",
    "        result = ' '.join([index_to_string[int(o)] for o in preds.data.numpy() if o!=1])\n",
    "        orig = ' '.join([index_to_string[o] for o in summary[:,k].data.cpu().numpy() if o!=1])\n",
    "        \n",
    "        scores = evaluator.get_scores(result, orig)\n",
    "        SCORE['rouge-2']['f_score'] += scores['rouge-2']['f']/no_of_example         #take average of corresponding \n",
    "        SCORE['rouge-2']['p_score'] += scores['rouge-2']['p']/no_of_example         #scores for final validation SCORE\n",
    "        SCORE['rouge-2']['r_score'] += scores['rouge-2']['r']/no_of_example\n",
    "        \n",
    "        SCORE['rouge-3']['f_score'] += scores['rouge-3']['f']/no_of_example\n",
    "        SCORE['rouge-3']['p_score'] += scores['rouge-3']['p']/no_of_example\n",
    "        SCORE['rouge-3']['r_score'] += scores['rouge-3']['r']/no_of_example \n",
    "        \n",
    "        SCORE['rouge-1']['f_score'] += scores['rouge-1']['f']/no_of_example\n",
    "        SCORE['rouge-1']['p_score'] += scores['rouge-1']['p']/no_of_example\n",
    "        SCORE['rouge-1']['r_score'] += scores['rouge-2']['r']/no_of_example\n",
    "        \n",
    "        SCORE['rouge-l']['f_score'] += scores['rouge-l']['f']/no_of_example\n",
    "        SCORE['rouge-l']['p_score'] += scores['rouge-l']['p']/no_of_example\n",
    "        SCORE['rouge-l']['r_score'] += scores['rouge-l']['r']/no_of_example\n",
    "        \n",
    "        SCORE['rouge-w']['f_score'] += scores['rouge-w']['f']/no_of_example\n",
    "        SCORE['rouge-w']['p_score'] += scores['rouge-w']['p']/no_of_example\n",
    "        SCORE['rouge-w']['r_score'] += scores['rouge-w']['r']/no_of_example\n",
    "        \n",
    "        \n",
    "        \n",
    "print('ROUGE SCORE ON VALIDATION SET')\n",
    "SCORE\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence : \n",
      "thailand will discuss lifting its ban on us beef , imposed two years ago over mad cow fears , when negotiators from the two nations meet next week for free trade talks , health officials said thursday . _eos_\n",
      "\n",
      "Result : \n",
      "thailand to to lifting beef beef chicken chicken chicken _eos_ _eos_ _eos_\n",
      "\n",
      "Original : \n",
      "thailand may lift ban on us beef _eos_\n"
     ]
    }
   ],
   "source": [
    "x,y = next(valid_iter_tuple.__iter__())\n",
    "out = model(y.transpose(1,0)[0].unsqueeze(1) , None , beam_width)     #out is list\n",
    "preds = out[0]\n",
    "\n",
    "sentence = ' '.join([index_to_string[o] for o in y[:,0].data.cpu().numpy() if o!=1])\n",
    "result = ' '.join([index_to_string[int(o)] for o in preds.data.numpy() if o!=1 ])\n",
    "orig = ' '.join([index_to_string[o] for o in x[:,0].data.cpu().numpy() if o!=1])\n",
    "\n",
    "print(\"\\nSentence : \\n%s\" %(sentence))\n",
    "print(\"\\nResult : \\n%s\" %(result))\n",
    "print(\"\\nOriginal : \\n%s\" %(orig))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE SCORES :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge-2': {'f': 0.888888888888889, 'p': 0.8, 'r': 1.0},\n",
       " 'rouge-1': {'f': 0.9090909090909091, 'p': 0.8333333333333334, 'r': 1.0},\n",
       " 'rouge-3': {'f': 0.5714285714285715, 'p': 0.5, 'r': 0.6666666666666666},\n",
       " 'rouge-l': {'f': 0.9241784847394879, 'p': 0.8590444340720371, 'r': 1.0},\n",
       " 'rouge-w': {'f': 0.7752750336123038,\n",
       "  'p': 0.8333333333333333,\n",
       "  'r': 0.7247796636776956}}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "scores = evaluator.get_scores(result, orig)\n",
    "print('ROUGE SCORES :') \n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "tensorflow_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
